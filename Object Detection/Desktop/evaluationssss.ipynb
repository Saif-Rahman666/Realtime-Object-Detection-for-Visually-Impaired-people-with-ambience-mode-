{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd0eb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ab1c6a0d3125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstandard_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoco_tools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_mask_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\raihan\\anaconda3\\envs\\obj_detection\\lib\\site-packages\\object_detection\\metrics\\coco_tools.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcocoeval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Class for evaluating object detections with COCO metrics.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import zip\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from object_detection.core import standard_fields\n",
    "from object_detection.metrics import coco_tools\n",
    "from object_detection.utils import json_utils\n",
    "from object_detection.utils import np_mask_ops\n",
    "from object_detection.utils import object_detection_evaluation\n",
    "\n",
    "\n",
    "class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):\n",
    "  \"\"\"Class to evaluate COCO detection metrics.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               categories,\n",
    "               include_metrics_per_category=False,\n",
    "               all_metrics_per_category=False,\n",
    "               skip_predictions_for_unlabeled_class=False,\n",
    "               super_categories=None):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      categories: A list of dicts, each of which has the following keys -\n",
    "        'id': (required) an integer id uniquely identifying this category.\n",
    "        'name': (required) string representing category name e.g., 'cat', 'dog'.\n",
    "      include_metrics_per_category: If True, include metrics for each category.\n",
    "      all_metrics_per_category: Whether to include all the summary metrics for\n",
    "        each category in per_category_ap. Be careful with setting it to true if\n",
    "        you have more than handful of categories, because it will pollute\n",
    "        your mldash.\n",
    "      skip_predictions_for_unlabeled_class: Skip predictions that do not match\n",
    "        with the labeled classes for the image.\n",
    "      super_categories: None or a python dict mapping super-category names\n",
    "        (strings) to lists of categories (corresponding to category names\n",
    "        in the label_map).  Metrics are aggregated along these super-categories\n",
    "        and added to the `per_category_ap` and are associated with the name\n",
    "          `PerformanceBySuperCategory/<super-category-name>`.\n",
    "    \"\"\"\n",
    "    super(CocoDetectionEvaluator, self).__init__(categories)\n",
    "    # _image_ids is a dictionary that maps unique image ids to Booleans which\n",
    "    # indicate whether a corresponding detection has been added.\n",
    "    self._image_ids = {}\n",
    "    self._groundtruth_list = []\n",
    "    self._detection_boxes_list = []\n",
    "    self._category_id_set = set([cat['id'] for cat in self._categories])\n",
    "    self._annotation_id = 1\n",
    "    self._metrics = None\n",
    "    self._include_metrics_per_category = include_metrics_per_category\n",
    "    self._all_metrics_per_category = all_metrics_per_category\n",
    "    self._skip_predictions_for_unlabeled_class = skip_predictions_for_unlabeled_class\n",
    "    self._groundtruth_labeled_classes = {}\n",
    "    self._super_categories = super_categories\n",
    "\n",
    "  def clear(self):\n",
    "    \"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"\n",
    "    self._image_ids.clear()\n",
    "    self._groundtruth_list = []\n",
    "    self._detection_boxes_list = []\n",
    "\n",
    "  def add_single_ground_truth_image_info(self,\n",
    "                                         image_id,\n",
    "                                         groundtruth_dict):\n",
    "    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n",
    "\n",
    "    If the image has already been added, a warning is logged, and groundtruth is\n",
    "    ignored.\n",
    "\n",
    "    Args:\n",
    "      image_id: A unique string/integer identifier for the image.\n",
    "      groundtruth_dict: A dictionary containing -\n",
    "        InputDataFields.groundtruth_boxes: float32 numpy array of shape\n",
    "          [num_boxes, 4] containing `num_boxes` groundtruth boxes of the format\n",
    "          [ymin, xmin, ymax, xmax] in absolute image coordinates.\n",
    "        InputDataFields.groundtruth_classes: integer numpy array of shape\n",
    "          [num_boxes] containing 1-indexed groundtruth classes for the boxes.\n",
    "        InputDataFields.groundtruth_is_crowd (optional): integer numpy array of\n",
    "          shape [num_boxes] containing iscrowd flag for groundtruth boxes.\n",
    "        InputDataFields.groundtruth_area (optional): float numpy array of\n",
    "          shape [num_boxes] containing the area (in the original absolute\n",
    "          coordinates) of the annotated object.\n",
    "        InputDataFields.groundtruth_keypoints (optional): float numpy array of\n",
    "          keypoints with shape [num_boxes, num_keypoints, 2].\n",
    "        InputDataFields.groundtruth_keypoint_visibilities (optional): integer\n",
    "          numpy array of keypoint visibilities with shape [num_gt_boxes,\n",
    "          num_keypoints]. Integer is treated as an enum with 0=not labeled,\n",
    "          1=labeled but not visible and 2=labeled and visible.\n",
    "        InputDataFields.groundtruth_labeled_classes (optional): a tensor of\n",
    "          shape [num_classes + 1] containing the multi-hot tensor indicating the\n",
    "          classes that each image is labeled for. Note that the classes labels\n",
    "          are 1-indexed.\n",
    "    \"\"\"\n",
    "    if image_id in self._image_ids:\n",
    "      tf.logging.warning('Ignoring ground truth with image id %s since it was '\n",
    "                         'previously added', image_id)\n",
    "      return\n",
    "\n",
    "    # Drop optional fields if empty tensor.\n",
    "    groundtruth_is_crowd = groundtruth_dict.get(\n",
    "        standard_fields.InputDataFields.groundtruth_is_crowd)\n",
    "    groundtruth_area = groundtruth_dict.get(\n",
    "        standard_fields.InputDataFields.groundtruth_area)\n",
    "    groundtruth_keypoints = groundtruth_dict.get(\n",
    "        standard_fields.InputDataFields.groundtruth_keypoints)\n",
    "    groundtruth_keypoint_visibilities = groundtruth_dict.get(\n",
    "        standard_fields.InputDataFields.groundtruth_keypoint_visibilities)\n",
    "    if groundtruth_is_crowd is not None and not groundtruth_is_crowd.shape[0]:\n",
    "      groundtruth_is_crowd = None\n",
    "    if groundtruth_area is not None and not groundtruth_area.shape[0]:\n",
    "      groundtruth_area = None\n",
    "    if groundtruth_keypoints is not None and not groundtruth_keypoints.shape[0]:\n",
    "      groundtruth_keypoints = None\n",
    "    if groundtruth_keypoint_visibilities is not None and not groundtruth_keypoint_visibilities.shape[\n",
    "        0]:\n",
    "      groundtruth_keypoint_visibilities = None\n",
    "\n",
    "    self._groundtruth_list.extend(\n",
    "        coco_tools.ExportSingleImageGroundtruthToCoco(\n",
    "            image_id=image_id,\n",
    "            next_annotation_id=self._annotation_id,\n",
    "            category_id_set=self._category_id_set,\n",
    "            groundtruth_boxes=groundtruth_dict[\n",
    "                standard_fields.InputDataFields.groundtruth_boxes],\n",
    "            groundtruth_classes=groundtruth_dict[\n",
    "                standard_fields.InputDataFields.groundtruth_classes],\n",
    "            groundtruth_is_crowd=groundtruth_is_crowd,\n",
    "            groundtruth_area=groundtruth_area,\n",
    "            groundtruth_keypoints=groundtruth_keypoints,\n",
    "            groundtruth_keypoint_visibilities=groundtruth_keypoint_visibilities)\n",
    "    )\n",
    "\n",
    "    self._annotation_id += groundtruth_dict[standard_fields.InputDataFields.\n",
    "                                            groundtruth_boxes].shape[0]\n",
    "    if (standard_fields.InputDataFields.groundtruth_labeled_classes\n",
    "       ) in groundtruth_dict:\n",
    "      labeled_classes = groundtruth_dict[\n",
    "          standard_fields.InputDataFields.groundtruth_labeled_classes]\n",
    "      if labeled_classes.shape != (len(self._category_id_set) + 1,):\n",
    "        raise ValueError('Invalid shape for groundtruth labeled classes: {}, '\n",
    "                         'num_categories_including_background: {}'.format(\n",
    "                             labeled_classes,\n",
    "                             len(self._category_id_set) + 1))\n",
    "      self._groundtruth_labeled_classes[image_id] = np.flatnonzero(\n",
    "          groundtruth_dict[standard_fields.InputDataFields\n",
    "                           .groundtruth_labeled_classes] == 1).tolist()\n",
    "\n",
    "    # Boolean to indicate whether a detection has been added for this image.\n",
    "    self._image_ids[image_id] = False\n",
    "\n",
    "  def add_single_detected_image_info(self,\n",
    "                                     image_id,\n",
    "                                     detections_dict):\n",
    "    \"\"\"Adds detections for a single image to be used for evaluation.\n",
    "\n",
    "    If a detection has already been added for this image id, a warning is\n",
    "    logged, and the detection is skipped.\n",
    "\n",
    "    Args:\n",
    "      image_id: A unique string/integer identifier for the image.\n",
    "      detections_dict: A dictionary containing -\n",
    "        DetectionResultFields.detection_boxes: float32 numpy array of shape\n",
    "          [num_boxes, 4] containing `num_boxes` detection boxes of the format\n",
    "          [ymin, xmin, ymax, xmax] in absolute image coordinates.\n",
    "        DetectionResultFields.detection_scores: float32 numpy array of shape\n",
    "          [num_boxes] containing detection scores for the boxes.\n",
    "        DetectionResultFields.detection_classes: integer numpy array of shape\n",
    "          [num_boxes] containing 1-indexed detection classes for the boxes.\n",
    "        DetectionResultFields.detection_keypoints (optional): float numpy array\n",
    "          of keypoints with shape [num_boxes, num_keypoints, 2].\n",
    "    Raises:\n",
    "      ValueError: If groundtruth for the image_id is not available.\n",
    "    \"\"\"\n",
    "    if image_id not in self._image_ids:\n",
    "      raise ValueError('Missing groundtruth for image id: {}'.format(image_id))\n",
    "\n",
    "    if self._image_ids[image_id]:\n",
    "      tf.logging.warning('Ignoring detection with image id %s since it was '\n",
    "                         'previously added', image_id)\n",
    "      return\n",
    "\n",
    "    # Drop optional fields if empty tensor.\n",
    "    detection_keypoints = detections_dict.get(\n",
    "        standard_fields.DetectionResultFields.detection_keypoints)\n",
    "    if detection_keypoints is not None and not detection_keypoints.shape[0]:\n",
    "      detection_keypoints = None\n",
    "\n",
    "    if self._skip_predictions_for_unlabeled_class:\n",
    "      det_classes = detections_dict[\n",
    "          standard_fields.DetectionResultFields.detection_classes]\n",
    "      num_det_boxes = det_classes.shape[0]\n",
    "      keep_box_ids = []\n",
    "      for box_id in range(num_det_boxes):\n",
    "        if det_classes[box_id] in self._groundtruth_labeled_classes[image_id]:\n",
    "          keep_box_ids.append(box_id)\n",
    "      self._detection_boxes_list.extend(\n",
    "          coco_tools.ExportSingleImageDetectionBoxesToCoco(\n",
    "              image_id=image_id,\n",
    "              category_id_set=self._category_id_set,\n",
    "              detection_boxes=detections_dict[\n",
    "                  standard_fields.DetectionResultFields.detection_boxes]\n",
    "              [keep_box_ids],\n",
    "              detection_scores=detections_dict[\n",
    "                  standard_fields.DetectionResultFields.detection_scores]\n",
    "              [keep_box_ids],\n",
    "              detection_classes=detections_dict[\n",
    "                  standard_fields.DetectionResultFields.detection_classes]\n",
    "              [keep_box_ids],\n",
    "              detection_keypoints=detection_keypoints))\n",
    "    else:\n",
    "      self._detection_boxes_list.extend(\n",
    "          coco_tools.ExportSingleImageDetectionBoxesToCoco(\n",
    "              image_id=image_id,\n",
    "              category_id_set=self._category_id_set,\n",
    "              detection_boxes=detections_dict[\n",
    "                  standard_fields.DetectionResultFields.detection_boxes],\n",
    "              detection_scores=detections_dict[\n",
    "                  standard_fields.DetectionResultFields.detection_scores],\n",
    "              detection_classes=detections_dict[\n",
    "                  standard_fields.DetectionResultFields.detection_classes],\n",
    "              detection_keypoints=detection_keypoints))\n",
    "    self._image_ids[image_id] = True\n",
    "\n",
    "  def dump_detections_to_json_file(self, json_output_path):\n",
    "    \"\"\"Saves the detections into json_output_path in the format used by MS COCO.\n",
    "\n",
    "    Args:\n",
    "      json_output_path: String containing the output file's path. It can be also\n",
    "        None. In that case nothing will be written to the output file.\n",
    "    \"\"\"\n",
    "    if json_output_path and json_output_path is not None:\n",
    "      with tf.gfile.GFile(json_output_path, 'w') as fid:\n",
    "        tf.logging.info('Dumping detections to output json file.')\n",
    "        json_utils.Dump(\n",
    "            obj=self._detection_boxes_list, fid=fid, float_digits=4, indent=2)\n",
    "\n",
    "  def evaluate(self):\n",
    "    \"\"\"Evaluates the detection boxes and returns a dictionary of coco metrics.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary holding -\n",
    "\n",
    "      1. summary_metrics:\n",
    "      'DetectionBoxes_Precision/mAP': mean average precision over classes\n",
    "        averaged over IOU thresholds ranging from .5 to .95 with .05\n",
    "        increments.\n",
    "      'DetectionBoxes_Precision/mAP@.50IOU': mean average precision at 50% IOU\n",
    "      'DetectionBoxes_Precision/mAP@.75IOU': mean average precision at 75% IOU\n",
    "      'DetectionBoxes_Precision/mAP (small)': mean average precision for small\n",
    "        objects (area < 32^2 pixels).\n",
    "      'DetectionBoxes_Precision/mAP (medium)': mean average precision for\n",
    "        medium sized objects (32^2 pixels < area < 96^2 pixels).\n",
    "      'DetectionBoxes_Precision/mAP (large)': mean average precision for large\n",
    "        objects (96^2 pixels < area < 10000^2 pixels).\n",
    "      'DetectionBoxes_Recall/AR@1': average recall with 1 detection.\n",
    "      'DetectionBoxes_Recall/AR@10': average recall with 10 detections.\n",
    "      'DetectionBoxes_Recall/AR@100': average recall with 100 detections.\n",
    "      'DetectionBoxes_Recall/AR@100 (small)': average recall for small objects\n",
    "        with 100.\n",
    "      'DetectionBoxes_Recall/AR@100 (medium)': average recall for medium objects\n",
    "        with 100.\n",
    "      'DetectionBoxes_Recall/AR@100 (large)': average recall for large objects\n",
    "        with 100 detections.\n",
    "\n",
    "      2. per_category_ap: if include_metrics_per_category is True, category\n",
    "      specific results with keys of the form:\n",
    "      'Precision mAP ByCategory/category' (without the supercategory part if\n",
    "      no supercategories exist). For backward compatibility\n",
    "      'PerformanceByCategory' is included in the output regardless of\n",
    "      all_metrics_per_category.\n",
    "        If super_categories are provided, then this will additionally include\n",
    "      metrics aggregated along the super_categories with keys of the form:\n",
    "      `PerformanceBySuperCategory/<super-category-name>`\n",
    "    \"\"\"\n",
    "    tf.logging.info('Performing evaluation on %d images.', len(self._image_ids))\n",
    "    groundtruth_dict = {\n",
    "        'annotations': self._groundtruth_list,\n",
    "        'images': [{'id': image_id} for image_id in self._image_ids],\n",
    "        'categories': self._categories\n",
    "    }\n",
    "    coco_wrapped_groundtruth = coco_tools.COCOWrapper(groundtruth_dict)\n",
    "    coco_wrapped_detections = coco_wrapped_groundtruth.LoadAnnotations(\n",
    "        self._detection_boxes_list)\n",
    "    box_evaluator = coco_tools.COCOEvalWrapper(\n",
    "        coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n",
    "    box_metrics, box_per_category_ap = box_evaluator.ComputeMetrics(\n",
    "        include_metrics_per_category=self._include_metrics_per_category,\n",
    "        all_metrics_per_category=self._all_metrics_per_category,\n",
    "        super_categories=self._super_categories)\n",
    "    box_metrics.update(box_per_category_ap)\n",
    "    box_metrics = {'DetectionBoxes_'+ key: value\n",
    "                   for key, value in iter(box_metrics.items())}\n",
    "    return box_metrics\n",
    "\n",
    "  def add_eval_dict(self, eval_dict):\n",
    "    \"\"\"Observes an evaluation result dict for a single example.\n",
    "\n",
    "    When executing eagerly, once all observations have been observed by this\n",
    "    method you can use `.evaluate()` to get the final metrics.\n",
    "\n",
    "    When using `tf.estimator.Estimator` for evaluation this function is used by\n",
    "    `get_estimator_eval_metric_ops()` to construct the metric update op.\n",
    "\n",
    "    Args:\n",
    "      eval_dict: A dictionary that holds tensors for evaluating an object\n",
    "        detection model, returned from\n",
    "        eval_util.result_dict_for_single_example().\n",
    "\n",
    "    Returns:\n",
    "      None when executing eagerly, or an update_op that can be used to update\n",
    "      the eval metrics in `tf.estimator.EstimatorSpec`.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_op(image_id_batched, groundtruth_boxes_batched,\n",
    "                  groundtruth_classes_batched, groundtruth_is_crowd_batched,\n",
    "                  groundtruth_labeled_classes_batched, num_gt_boxes_per_image,\n",
    "                  detection_boxes_batched, detection_scores_batched,\n",
    "                  detection_classes_batched, num_det_boxes_per_image,\n",
    "                  is_annotated_batched):\n",
    "      \"\"\"Update operation for adding batch of images to Coco evaluator.\"\"\"\n",
    "      for (image_id, gt_box, gt_class, gt_is_crowd, gt_labeled_classes,\n",
    "           num_gt_box, det_box, det_score, det_class,\n",
    "           num_det_box, is_annotated) in zip(\n",
    "               image_id_batched, groundtruth_boxes_batched,\n",
    "               groundtruth_classes_batched, groundtruth_is_crowd_batched,\n",
    "               groundtruth_labeled_classes_batched, num_gt_boxes_per_image,\n",
    "               detection_boxes_batched, detection_scores_batched,\n",
    "               detection_classes_batched, num_det_boxes_per_image,\n",
    "               is_annotated_batched):\n",
    "        if is_annotated:\n",
    "          self.add_single_ground_truth_image_info(\n",
    "              image_id, {\n",
    "                  'groundtruth_boxes': gt_box[:num_gt_box],\n",
    "                  'groundtruth_classes': gt_class[:num_gt_box],\n",
    "                  'groundtruth_is_crowd': gt_is_crowd[:num_gt_box],\n",
    "                  'groundtruth_labeled_classes': gt_labeled_classes\n",
    "              })\n",
    "          self.add_single_detected_image_info(\n",
    "              image_id,\n",
    "              {'detection_boxes': det_box[:num_det_box],\n",
    "               'detection_scores': det_score[:num_det_box],\n",
    "               'detection_classes': det_class[:num_det_box]})\n",
    "\n",
    "    # Unpack items from the evaluation dictionary.\n",
    "    input_data_fields = standard_fields.InputDataFields\n",
    "    detection_fields = standard_fields.DetectionResultFields\n",
    "    image_id = eval_dict[input_data_fields.key]\n",
    "    groundtruth_boxes = eval_dict[input_data_fields.groundtruth_boxes]\n",
    "    groundtruth_classes = eval_dict[input_data_fields.groundtruth_classes]\n",
    "    groundtruth_is_crowd = eval_dict.get(\n",
    "        input_data_fields.groundtruth_is_crowd, None)\n",
    "    groundtruth_labeled_classes = eval_dict.get(\n",
    "        input_data_fields.groundtruth_labeled_classes, None)\n",
    "    detection_boxes = eval_dict[detection_fields.detection_boxes]\n",
    "    detection_scores = eval_dict[detection_fields.detection_scores]\n",
    "    detection_classes = eval_dict[detection_fields.detection_classes]\n",
    "    num_gt_boxes_per_image = eval_dict.get(\n",
    "        input_data_fields.num_groundtruth_boxes, None)\n",
    "    num_det_boxes_per_image = eval_dict.get(detection_fields.num_detections,\n",
    "                                            None)\n",
    "    is_annotated = eval_dict.get('is_annotated', None)\n",
    "\n",
    "    if groundtruth_is_crowd is None:\n",
    "      groundtruth_is_crowd = tf.zeros_like(groundtruth_classes, dtype=tf.bool)\n",
    "\n",
    "    # If groundtruth_labeled_classes is not provided, make it equal to the\n",
    "    # detection_classes. This assumes that all predictions will be kept to\n",
    "    # compute eval metrics.\n",
    "    if groundtruth_labeled_classes is None:\n",
    "      groundtruth_labeled_classes = tf.reduce_max(\n",
    "          tf.one_hot(\n",
    "              tf.cast(detection_classes, tf.int32),\n",
    "              len(self._category_id_set) + 1),\n",
    "          axis=-2)\n",
    "\n",
    "    if not image_id.shape.as_list():\n",
    "      # Apply a batch dimension to all tensors.\n",
    "      image_id = tf.expand_dims(image_id, 0)\n",
    "      groundtruth_boxes = tf.expand_dims(groundtruth_boxes, 0)\n",
    "      groundtruth_classes = tf.expand_dims(groundtruth_classes, 0)\n",
    "      groundtruth_is_crowd = tf.expand_dims(groundtruth_is_crowd, 0)\n",
    "      groundtruth_labeled_classes = tf.expand_dims(groundtruth_labeled_classes,\n",
    "                                                   0)\n",
    "      detection_boxes = tf.expand_dims(detection_boxes, 0)\n",
    "      detection_scores = tf.expand_dims(detection_scores, 0)\n",
    "      detection_classes = tf.expand_dims(detection_classes, 0)\n",
    "\n",
    "      if num_gt_boxes_per_image is None:\n",
    "        num_gt_boxes_per_image = tf.shape(groundtruth_boxes)[1:2]\n",
    "      else:\n",
    "        num_gt_boxes_per_image = tf.expand_dims(num_gt_boxes_per_image, 0)\n",
    "\n",
    "      if num_det_boxes_per_image is None:\n",
    "        num_det_boxes_per_image = tf.shape(detection_boxes)[1:2]\n",
    "      else:\n",
    "        num_det_boxes_per_image = tf.expand_dims(num_det_boxes_per_image, 0)\n",
    "\n",
    "      if is_annotated is None:\n",
    "        is_annotated = tf.constant([True])\n",
    "      else:\n",
    "        is_annotated = tf.expand_dims(is_annotated, 0)\n",
    "    else:\n",
    "      if num_gt_boxes_per_image is None:\n",
    "        num_gt_boxes_per_image = tf.tile(\n",
    "            tf.shape(groundtruth_boxes)[1:2],\n",
    "            multiples=tf.shape(groundtruth_boxes)[0:1])\n",
    "      if num_det_boxes_per_image is None:\n",
    "        num_det_boxes_per_image = tf.tile(\n",
    "            tf.shape(detection_boxes)[1:2],\n",
    "            multiples=tf.shape(detection_boxes)[0:1])\n",
    "      if is_annotated is None:\n",
    "        is_annotated = tf.ones_like(image_id, dtype=tf.bool)\n",
    "\n",
    "    return tf.py_func(update_op, [\n",
    "        image_id, groundtruth_boxes, groundtruth_classes, groundtruth_is_crowd,\n",
    "        groundtruth_labeled_classes, num_gt_boxes_per_image, detection_boxes,\n",
    "        detection_scores, detection_classes, num_det_boxes_per_image,\n",
    "        is_annotated\n",
    "    ], [])\n",
    "\n",
    "  def get_estimator_eval_metric_ops(self, eval_dict):\n",
    "    \"\"\"Returns a dictionary of eval metric ops.\n",
    "\n",
    "    Note that once value_op is called, the detections and groundtruth added via\n",
    "    update_op are cleared.\n",
    "\n",
    "    This function can take in groundtruth and detections for a batch of images,\n",
    "    or for a single image. For the latter case, the batch dimension for input\n",
    "    tensors need not be present.\n",
    "\n",
    "    Args:\n",
    "      eval_dict: A dictionary that holds tensors for evaluating object detection\n",
    "        performance. For single-image evaluation, this dictionary may be\n",
    "        produced from eval_util.result_dict_for_single_example(). If multi-image\n",
    "        evaluation, `eval_dict` should contain the fields\n",
    "        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to\n",
    "        properly unpad the tensors from the batch.\n",
    "\n",
    "    Returns:\n",
    "      a dictionary of metric names to tuple of value_op and update_op that can\n",
    "      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all\n",
    "      update ops must be run together and similarly all value ops must be run\n",
    "      together to guarantee correct behaviour.\n",
    "    \"\"\"\n",
    "    update_op = self.add_eval_dict(eval_dict)\n",
    "    metric_names = ['DetectionBoxes_Precision/mAP',\n",
    "                    'DetectionBoxes_Precision/mAP@.50IOU',\n",
    "                    'DetectionBoxes_Precision/mAP@.75IOU',\n",
    "                    'DetectionBoxes_Precision/mAP (large)',\n",
    "                    'DetectionBoxes_Precision/mAP (medium)',\n",
    "                    'DetectionBoxes_Precision/mAP (small)',\n",
    "                    'DetectionBoxes_Recall/AR@1',\n",
    "                    'DetectionBoxes_Recall/AR@10',\n",
    "                    'DetectionBoxes_Recall/AR@100',\n",
    "                    'DetectionBoxes_Recall/AR@100 (large)',\n",
    "                    'DetectionBoxes_Recall/AR@100 (medium)',\n",
    "                    'DetectionBoxes_Recall/AR@100 (small)']\n",
    "    if self._include_metrics_per_category:\n",
    "      for category_dict in self._categories:\n",
    "        metric_names.append('DetectionBoxes_PerformanceByCategory/mAP/' +\n",
    "                            category_dict['name'])\n",
    "\n",
    "    def first_value_func():\n",
    "      self._metrics = self.evaluate()\n",
    "      self.clear()\n",
    "      return np.float32(self._metrics[metric_names[0]])\n",
    "\n",
    "    def value_func_factory(metric_name):\n",
    "      def value_func():\n",
    "        return np.float32(self._metrics[metric_name])\n",
    "      return value_func\n",
    "\n",
    "    # Ensure that the metrics are only evaluated once.\n",
    "    first_value_op = tf.py_func(first_value_func, [], tf.float32)\n",
    "    eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}\n",
    "    with tf.control_dependencies([first_value_op]):\n",
    "      for metric_name in metric_names[1:]:\n",
    "        eval_metric_ops[metric_name] = (tf.py_func(\n",
    "            value_func_factory(metric_name), [], np.float32), update_op)\n",
    "    return eval_metric_ops\n",
    "\n",
    "\n",
    "def convert_masks_to_binary(masks):\n",
    "  \"\"\"Converts masks to 0 or 1 and uint8 type.\"\"\"\n",
    "  return (masks > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "class CocoKeypointEvaluator(CocoDetectionEvaluator):\n",
    "  \"\"\"Class to evaluate COCO keypoint metrics.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               category_id,\n",
    "               category_keypoints,\n",
    "               class_text,\n",
    "               oks_sigmas=None):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      category_id: An integer id uniquely identifying this category.\n",
    "      category_keypoints: A list specifying keypoint mappings, with items:\n",
    "          'id': (required) an integer id identifying the keypoint.\n",
    "          'name': (required) a string representing the keypoint name.\n",
    "      class_text: A string representing the category name for which keypoint\n",
    "        metrics are to be computed.\n",
    "      oks_sigmas: A dict of keypoint name to standard deviation values for OKS\n",
    "        metrics. If not provided, default value of 0.05 will be used.\n",
    "    \"\"\"\n",
    "    self._category_id = category_id\n",
    "    self._category_name = class_text\n",
    "    self._keypoint_ids = sorted(\n",
    "        [keypoint['id'] for keypoint in category_keypoints])\n",
    "    kpt_id_to_name = {kpt['id']: kpt['name'] for kpt in category_keypoints}\n",
    "    if oks_sigmas:\n",
    "      self._oks_sigmas = np.array([\n",
    "          oks_sigmas[kpt_id_to_name[idx]] for idx in self._keypoint_ids\n",
    "      ])\n",
    "    else:\n",
    "      # Default all per-keypoint sigmas to 0.\n",
    "      self._oks_sigmas = np.full((len(self._keypoint_ids)), 0.05)\n",
    "      tf.logging.warning('No default keypoint OKS sigmas provided. Will use '\n",
    "                         '0.05')\n",
    "    tf.logging.info('Using the following keypoint OKS sigmas: {}'.format(\n",
    "        self._oks_sigmas))\n",
    "    self._metrics = None\n",
    "    super(CocoKeypointEvaluator, self).__init__([{\n",
    "        'id': self._category_id,\n",
    "        'name': class_text\n",
    "    }])\n",
    "\n",
    "  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n",
    "    \"\"\"Adds groundtruth for a single image with keypoints.\n",
    "\n",
    "    If the image has already been added, a warning is logged, and groundtruth\n",
    "    is ignored.\n",
    "\n",
    "    Args:\n",
    "      image_id: A unique string/integer identifier for the image.\n",
    "      groundtruth_dict: A dictionary containing -\n",
    "        InputDataFields.groundtruth_boxes: float32 numpy array of shape\n",
    "          [num_boxes, 4] containing `num_boxes` groundtruth boxes of the format\n",
    "          [ymin, xmin, ymax, xmax] in absolute image coordinates.\n",
    "        InputDataFields.groundtruth_classes: integer numpy array of shape\n",
    "          [num_boxes] containing 1-indexed groundtruth classes for the boxes.\n",
    "        InputDataFields.groundtruth_is_crowd (optional): integer numpy array of\n",
    "          shape [num_boxes] containing iscrowd flag for groundtruth boxes.\n",
    "        InputDataFields.groundtruth_area (optional): float numpy array of\n",
    "          shape [num_boxes] containing the area (in the original absolute\n",
    "          coordinates) of the annotated object.\n",
    "        InputDataFields.groundtruth_keypoints: float numpy array of\n",
    "          keypoints with shape [num_boxes, num_keypoints, 2].\n",
    "        InputDataFields.groundtruth_keypoint_visibilities (optional): integer\n",
    "          numpy array of keypoint visibilities with shape [num_gt_boxes,\n",
    "          num_keypoints]. Integer is treated as an enum with 0=not labels,\n",
    "          1=labeled but not visible and 2=labeled and visible.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep only the groundtruth for our category and its keypoints.\n",
    "    groundtruth_classes = groundtruth_dict[\n",
    "        standard_fields.InputDataFields.groundtruth_classes]\n",
    "    groundtruth_boxes = groundtruth_dict[\n",
    "        standard_fields.InputDataFields.groundtruth_boxes]\n",
    "    groundtruth_keypoints = groundtruth_dict[\n",
    "        standard_fields.InputDataFields.groundtruth_keypoints]\n",
    "    class_indices = [\n",
    "        idx for idx, gt_class_id in enumerate(groundtruth_classes)\n",
    "        if gt_class_id == self._category_id\n",
    "    ]\n",
    "    filtered_groundtruth_classes = np.take(\n",
    "        groundtruth_classes, class_indices, axis=0)\n",
    "    filtered_groundtruth_boxes = np.take(\n",
    "        groundtruth_boxes, class_indices, axis=0)\n",
    "    filtered_groundtruth_keypoints = np.take(\n",
    "        groundtruth_keypoints, class_indices, axis=0)\n",
    "    filtered_groundtruth_keypoints = np.take(\n",
    "        filtered_groundtruth_keypoints, self._keypoint_ids, axis=1)\n",
    "\n",
    "    filtered_groundtruth_dict = {}\n",
    "    filtered_groundtruth_dict[\n",
    "        standard_fields.InputDataFields\n",
    "        .groundtruth_classes] = filtered_groundtruth_classes\n",
    "    filtered_groundtruth_dict[standard_fields.InputDataFields\n",
    "                              .groundtruth_boxes] = filtered_groundtruth_boxes\n",
    "    filtered_groundtruth_dict[\n",
    "        standard_fields.InputDataFields\n",
    "        .groundtruth_keypoints] = filtered_groundtruth_keypoints\n",
    "\n",
    "    if (standard_fields.InputDataFields.groundtruth_is_crowd in\n",
    "        groundtruth_dict.keys()):\n",
    "      groundtruth_is_crowd = groundtruth_dict[\n",
    "          standard_fields.InputDataFields.groundtruth_is_crowd]\n",
    "      filtered_groundtruth_is_crowd = np.take(groundtruth_is_crowd,\n",
    "                                              class_indices, 0)\n",
    "      filtered_groundtruth_dict[\n",
    "          standard_fields.InputDataFields\n",
    "          .groundtruth_is_crowd] = filtered_groundtruth_is_crowd\n",
    "    if (standard_fields.InputDataFields.groundtruth_area in\n",
    "        groundtruth_dict.keys()):\n",
    "      groundtruth_area = groundtruth_dict[\n",
    "          standard_fields.InputDataFields.groundtruth_area]\n",
    "      filtered_groundtruth_area = np.take(groundtruth_area, class_indices, 0)\n",
    "      filtered_groundtruth_dict[\n",
    "          standard_fields.InputDataFields\n",
    "          .groundtruth_area] = filtered_groundtruth_area\n",
    "    if (standard_fields.InputDataFields.groundtruth_keypoint_visibilities in\n",
    "        groundtruth_dict.keys()):\n",
    "      groundtruth_keypoint_visibilities = groundtruth_dict[\n",
    "          standard_fields.InputDataFields.groundtruth_keypoint_visibilities]\n",
    "      filtered_groundtruth_keypoint_visibilities = np.take(\n",
    "          groundtruth_keypoint_visibilities, class_indices, axis=0)\n",
    "      filtered_groundtruth_keypoint_visibilities = np.take(\n",
    "          filtered_groundtruth_keypoint_visibilities,\n",
    "          self._keypoint_ids,\n",
    "          axis=1)\n",
    "      filtered_groundtruth_dict[\n",
    "          standard_fields.InputDataFields.\n",
    "          groundtruth_keypoint_visibilities] = filtered_groundtruth_keypoint_visibilities\n",
    "\n",
    "    super(CocoKeypointEvaluator,\n",
    "          self).add_single_ground_truth_image_info(image_id,\n",
    "                                                   filtered_groundtruth_dict)\n",
    "\n",
    "  def add_single_detected_image_info(self, image_id, detections_dict):\n",
    "    \"\"\"Adds detections for a single image and the specific category for which keypoints are evaluated.\n",
    "\n",
    "    If a detection has already been added for this image id, a warning is\n",
    "    logged, and the detection is skipped.\n",
    "\n",
    "    Args:\n",
    "      image_id: A unique string/integer identifier for the image.\n",
    "      detections_dict: A dictionary containing -\n",
    "        DetectionResultFields.detection_boxes: float32 numpy array of shape\n",
    "          [num_boxes, 4] containing `num_boxes` detection boxes of the format\n",
    "          [ymin, xmin, ymax, xmax] in absolute image coordinates.\n",
    "        DetectionResultFields.detection_scores: float32 numpy array of shape\n",
    "          [num_boxes] containing detection scores for the boxes.\n",
    "        DetectionResultFields.detection_classes: integer numpy array of shape\n",
    "          [num_boxes] containing 1-indexed detection classes for the boxes.\n",
    "        DetectionResultFields.detection_keypoints: float numpy array of\n",
    "          keypoints with shape [num_boxes, num_keypoints, 2].\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If groundtruth for the image_id is not available.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep only the detections for our category and its keypoints.\n",
    "    detection_classes = detections_dict[\n",
    "        standard_fields.DetectionResultFields.detection_classes]\n",
    "    detection_boxes = detections_dict[\n",
    "        standard_fields.DetectionResultFields.detection_boxes]\n",
    "    detection_scores = detections_dict[\n",
    "        standard_fields.DetectionResultFields.detection_scores]\n",
    "    detection_keypoints = detections_dict[\n",
    "        standard_fields.DetectionResultFields.detection_keypoints]\n",
    "    class_indices = [\n",
    "        idx for idx, class_id in enumerate(detection_classes)\n",
    "        if class_id == self._category_id\n",
    "    ]\n",
    "    filtered_detection_classes = np.take(\n",
    "        detection_classes, class_indices, axis=0)\n",
    "    filtered_detection_boxes = np.take(detection_boxes, class_indices, axis=0)\n",
    "    filtered_detection_scores = np.take(detection_scores, class_indices, axis=0)\n",
    "    filtered_detection_keypoints = np.take(\n",
    "        detection_keypoints, class_indices, axis=0)\n",
    "    filtered_detection_keypoints = np.take(\n",
    "        filtered_detection_keypoints, self._keypoint_ids, axis=1)\n",
    "\n",
    "    filtered_detections_dict = {}\n",
    "    filtered_detections_dict[standard_fields.DetectionResultFields\n",
    "                             .detection_classes] = filtered_detection_classes\n",
    "    filtered_detections_dict[standard_fields.DetectionResultFields\n",
    "                             .detection_boxes] = filtered_detection_boxes\n",
    "    filtered_detections_dict[standard_fields.DetectionResultFields\n",
    "                             .detection_scores] = filtered_detection_scores\n",
    "    filtered_detections_dict[standard_fields.DetectionResultFields.\n",
    "                             detection_keypoints] = filtered_detection_keypoints\n",
    "\n",
    "    super(CocoKeypointEvaluator,\n",
    "          self).add_single_detected_image_info(image_id,\n",
    "                                               filtered_detections_dict)\n",
    "\n",
    "  def evaluate(self):\n",
    "    \"\"\"Evaluates the keypoints and returns a dictionary of coco metrics.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary holding -\n",
    "\n",
    "      1. summary_metrics:\n",
    "      'Keypoints_Precision/mAP': mean average precision over classes\n",
    "        averaged over OKS thresholds ranging from .5 to .95 with .05\n",
    "        increments.\n",
    "      'Keypoints_Precision/mAP@.50IOU': mean average precision at 50% OKS\n",
    "      'Keypoints_Precision/mAP@.75IOU': mean average precision at 75% OKS\n",
    "      'Keypoints_Precision/mAP (medium)': mean average precision for medium\n",
    "        sized objects (32^2 pixels < area < 96^2 pixels).\n",
    "      'Keypoints_Precision/mAP (large)': mean average precision for large\n",
    "        objects (96^2 pixels < area < 10000^2 pixels).\n",
    "      'Keypoints_Recall/AR@1': average recall with 1 detection.\n",
    "      'Keypoints_Recall/AR@10': average recall with 10 detections.\n",
    "      'Keypoints_Recall/AR@100': average recall with 100 detections.\n",
    "      'Keypoints_Recall/AR@100 (medium)': average recall for medium objects with\n",
    "        100.\n",
    "      'Keypoints_Recall/AR@100 (large)': average recall for large objects with\n",
    "        100 detections.\n",
    "    \"\"\"\n",
    "    tf.logging.info('Performing evaluation on %d images.', len(self._image_ids))\n",
    "    groundtruth_dict = {\n",
    "        'annotations': self._groundtruth_list,\n",
    "        'images': [{'id': image_id} for image_id in self._image_ids],\n",
    "        'categories': self._categories\n",
    "    }\n",
    "    coco_wrapped_groundtruth = coco_tools.COCOWrapper(\n",
    "        groundtruth_dict, detection_type='bbox')\n",
    "    coco_wrapped_detections = coco_wrapped_groundtruth.LoadAnnotations(\n",
    "        self._detection_boxes_list)\n",
    "    keypoint_evaluator = coco_tools.COCOEvalWrapper(\n",
    "        coco_wrapped_groundtruth,\n",
    "        coco_wrapped_detections,\n",
    "        agnostic_mode=False,\n",
    "        iou_type='keypoints',\n",
    "        oks_sigmas=self._oks_sigmas)\n",
    "    keypoint_metrics, _ = keypoint_evaluator.ComputeMetrics(\n",
    "        include_metrics_per_category=False, all_metrics_per_category=False)\n",
    "    keypoint_metrics = {\n",
    "        'Keypoints_' + key: value\n",
    "        for key, value in iter(keypoint_metrics.items())\n",
    "    }\n",
    "    return keypoint_metrics\n",
    "\n",
    "  def add_eval_dict(self, eval_dict):\n",
    "    \"\"\"Observes an evaluation result dict for a single example.\n",
    "\n",
    "    When executing eagerly, once all observations have been observed by this\n",
    "    method you can use `.evaluate()` to get the final metrics.\n",
    "\n",
    "    When using `tf.estimator.Estimator` for evaluation this function is used by\n",
    "    `get_estimator_eval_metric_ops()` to construct the metric update op.\n",
    "\n",
    "    Args:\n",
    "      eval_dict: A dictionary that holds tensors for evaluating an object\n",
    "        detection model, returned from\n",
    "        eval_util.result_dict_for_single_example().\n",
    "\n",
    "    Returns:\n",
    "      None when executing eagerly, or an update_op that can be used to update\n",
    "      the eval metrics in `tf.estimator.EstimatorSpec`.\n",
    "    \"\"\"\n",
    "    def update_op(\n",
    "        image_id_batched,\n",
    "        groundtruth_boxes_batched,\n",
    "        groundtruth_classes_batched,\n",
    "        groundtruth_is_crowd_batched,\n",
    "        groundtruth_area_batched,\n",
    "        groundtruth_keypoints_batched,\n",
    "        groundtruth_keypoint_visibilities_batched,\n",
    "        num_gt_boxes_per_image,\n",
    "        detection_boxes_batched,\n",
    "        detection_scores_batched,\n",
    "        detection_classes_batched,\n",
    "        detection_keypoints_batched,\n",
    "        num_det_boxes_per_image,\n",
    "        is_annotated_batched):\n",
    "      \"\"\"Update operation for adding batch of images to Coco evaluator.\"\"\"\n",
    "\n",
    "      for (image_id, gt_box, gt_class, gt_is_crowd, gt_area, gt_keyp,\n",
    "           gt_keyp_vis, num_gt_box, det_box, det_score, det_class, det_keyp,\n",
    "           num_det_box, is_annotated) in zip(\n",
    "               image_id_batched, groundtruth_boxes_batched,\n",
    "               groundtruth_classes_batched, groundtruth_is_crowd_batched,\n",
    "               groundtruth_area_batched, groundtruth_keypoints_batched,\n",
    "               groundtruth_keypoint_visibilities_batched,\n",
    "               num_gt_boxes_per_image, detection_boxes_batched,\n",
    "               detection_scores_batched, detection_classes_batched,\n",
    "               detection_keypoints_batched, num_det_boxes_per_image,\n",
    "               is_annotated_batched):\n",
    "        if is_annotated:\n",
    "          self.add_single_ground_truth_image_info(\n",
    "              image_id, {\n",
    "                  'groundtruth_boxes': gt_box[:num_gt_box],\n",
    "                  'groundtruth_classes': gt_class[:num_gt_box],\n",
    "                  'groundtruth_is_crowd': gt_is_crowd[:num_gt_box],\n",
    "                  'groundtruth_area': gt_area[:num_gt_box],\n",
    "                  'groundtruth_keypoints': gt_keyp[:num_gt_box],\n",
    "                  'groundtruth_keypoint_visibilities': gt_keyp_vis[:num_gt_box]\n",
    "              })\n",
    "          self.add_single_detected_image_info(\n",
    "              image_id, {\n",
    "                  'detection_boxes': det_box[:num_det_box],\n",
    "                  'detection_scores': det_score[:num_det_box],\n",
    "                  'detection_classes': det_class[:num_det_box],\n",
    "                  'detection_keypoints': det_keyp[:num_det_box],\n",
    "              })\n",
    "\n",
    "    # Unpack items from the evaluation dictionary.\n",
    "    input_data_fields = standard_fields.InputDataFields\n",
    "    detection_fields = standard_fields.DetectionResultFields\n",
    "    image_id = eval_dict[input_data_fields.key]\n",
    "    groundtruth_boxes = eval_dict[input_data_fields.groundtruth_boxes]\n",
    "    groundtruth_classes = eval_dict[input_data_fields.groundtruth_classes]\n",
    "    groundtruth_is_crowd = eval_dict.get(input_data_fields.groundtruth_is_crowd,\n",
    "                                         None)\n",
    "    groundtruth_area = eval_dict.get(input_data_fields.groundtruth_area, None)\n",
    "    groundtruth_keypoints = eval_dict[input_data_fields.groundtruth_keypoints]\n",
    "    groundtruth_keypoint_visibilities = eval_dict.get(\n",
    "        input_data_fields.groundtruth_keypoint_visibilities, None)\n",
    "    detection_boxes = eval_dict[detection_fields.detection_boxes]\n",
    "    detection_scores = eval_dict[detection_fields.detection_scores]\n",
    "    detection_classes = eval_dict[detection_fields.detection_classes]\n",
    "    detection_keypoints = eval_dict[detection_fields.detection_keypoints]\n",
    "    num_gt_boxes_per_image = eval_dict.get(\n",
    "        'num_groundtruth_boxes_per_image', None)\n",
    "    num_det_boxes_per_image = eval_dict.get('num_det_boxes_per_image', None)\n",
    "    is_annotated = eval_dict.get('is_annotated', None)\n",
    "\n",
    "    if groundtruth_is_crowd is None:\n",
    "      groundtruth_is_crowd = tf.zeros_like(groundtruth_classes, dtype=tf.bool)\n",
    "\n",
    "    if groundtruth_area is None:\n",
    "      groundtruth_area = tf.zeros_like(groundtruth_classes, dtype=tf.float32)\n",
    "\n",
    "    if not image_id.shape.as_list():\n",
    "      # Apply a batch dimension to all tensors.\n",
    "      image_id = tf.expand_dims(image_id, 0)\n",
    "      groundtruth_boxes = tf.expand_dims(groundtruth_boxes, 0)\n",
    "      groundtruth_classes = tf.expand_dims(groundtruth_classes, 0)\n",
    "      groundtruth_is_crowd = tf.expand_dims(groundtruth_is_crowd, 0)\n",
    "      groundtruth_area = tf.expand_dims(groundtruth_area, 0)\n",
    "      groundtruth_keypoints = tf.expand_dims(groundtruth_keypoints, 0)\n",
    "      detection_boxes = tf.expand_dims(detection_boxes, 0)\n",
    "      detection_scores = tf.expand_dims(detection_scores, 0)\n",
    "      detection_classes = tf.expand_dims(detection_classes, 0)\n",
    "      detection_keypoints = tf.expand_dims(detection_keypoints, 0)\n",
    "\n",
    "      if num_gt_boxes_per_image is None:\n",
    "        num_gt_boxes_per_image = tf.shape(groundtruth_boxes)[1:2]\n",
    "      else:\n",
    "        num_gt_boxes_per_image = tf.expand_dims(num_gt_boxes_per_image, 0)\n",
    "\n",
    "      if num_det_boxes_per_image is None:\n",
    "        num_det_boxes_per_image = tf.shape(detection_boxes)[1:2]\n",
    "      else:\n",
    "        num_det_boxes_per_image = tf.expand_dims(num_det_boxes_per_image, 0)\n",
    "\n",
    "      if is_annotated is None:\n",
    "        is_annotated = tf.constant([True])\n",
    "      else:\n",
    "        is_annotated = tf.expand_dims(is_annotated, 0)\n",
    "\n",
    "      if groundtruth_keypoint_visibilities is None:\n",
    "        groundtruth_keypoint_visibilities = tf.fill([\n",
    "            tf.shape(groundtruth_boxes)[1],\n",
    "            tf.shape(groundtruth_keypoints)[2]\n",
    "        ], tf.constant(2, dtype=tf.int32))\n",
    "      groundtruth_keypoint_visibilities = tf.expand_dims(\n",
    "          groundtruth_keypoint_visibilities, 0)\n",
    "    else:\n",
    "      if num_gt_boxes_per_image is None:\n",
    "        num_gt_boxes_per_image = tf.tile(\n",
    "            tf.shape(groundtruth_boxes)[1:2],\n",
    "            multiples=tf.shape(groundtruth_boxes)[0:1])\n",
    "      if num_det_boxes_per_image is None:\n",
    "        num_det_boxes_per_image = tf.tile(\n",
    "            tf.shape(detection_boxes)[1:2],\n",
    "            multiples=tf.shape(detection_boxes)[0:1])\n",
    "      if is_annotated is None:\n",
    "        is_annotated = tf.ones_like(image_id, dtype=tf.bool)\n",
    "      if groundtruth_keypoint_visibilities is None:\n",
    "        groundtruth_keypoint_visibilities = tf.fill([\n",
    "            tf.shape(groundtruth_keypoints)[1],\n",
    "            tf.shape(groundtruth_keypoints)[2]\n",
    "        ], tf.constant(2, dtype=tf.int32))\n",
    "        groundtruth_keypoint_visibilities = tf.tile(\n",
    "            tf.expand_dims(groundtruth_keypoint_visibilities, 0),\n",
    "            multiples=[tf.shape(groundtruth_keypoints)[0], 1, 1])\n",
    "\n",
    "    return tf.py_func(update_op, [\n",
    "        image_id, groundtruth_boxes, groundtruth_classes, groundtruth_is_crowd,\n",
    "        groundtruth_area, groundtruth_keypoints,\n",
    "        groundtruth_keypoint_visibilities, num_gt_boxes_per_image,\n",
    "        detection_boxes, detection_scores, detection_classes,\n",
    "        detection_keypoints, num_det_boxes_per_image, is_annotated\n",
    "    ], [])\n",
    "\n",
    "  def get_estimator_eval_metric_ops(self, eval_dict):\n",
    "    \"\"\"Returns a dictionary of eval metric ops.\n",
    "\n",
    "    Note that once value_op is called, the detections and groundtruth added via\n",
    "    update_op are cleared.\n",
    "\n",
    "    This function can take in groundtruth and detections for a batch of images,\n",
    "    or for a single image. For the latter case, the batch dimension for input\n",
    "    tensors need not be present.\n",
    "\n",
    "    Args:\n",
    "      eval_dict: A dictionary that holds tensors for evaluating object detection\n",
    "        performance. For single-image evaluation, this dictionary may be\n",
    "        produced from eval_util.result_dict_for_single_example(). If multi-image\n",
    "        evaluation, `eval_dict` should contain the fields\n",
    "        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to\n",
    "        properly unpad the tensors from the batch.\n",
    "\n",
    "    Returns:\n",
    "      a dictionary of metric names to tuple of value_op and update_op that can\n",
    "      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all\n",
    "      update ops must be run together and similarly all value ops must be run\n",
    "      together to guarantee correct behaviour.\n",
    "    \"\"\"\n",
    "    update_op = self.add_eval_dict(eval_dict)\n",
    "    category = self._category_name\n",
    "    metric_names = [\n",
    "        'Keypoints_Precision/mAP ByCategory/{}'.format(category),\n",
    "        'Keypoints_Precision/mAP@.50IOU ByCategory/{}'.format(category),\n",
    "        'Keypoints_Precision/mAP@.75IOU ByCategory/{}'.format(category),\n",
    "        'Keypoints_Precision/mAP (large) ByCategory/{}'.format(category),\n",
    "        'Keypoints_Precision/mAP (medium) ByCategory/{}'.format(category),\n",
    "        'Keypoints_Recall/AR@1 ByCategory/{}'.format(category),\n",
    "        'Keypoints_Recall/AR@10 ByCategory/{}'.format(category),\n",
    "        'Keypoints_Recall/AR@100 ByCategory/{}'.format(category),\n",
    "        'Keypoints_Recall/AR@100 (large) ByCategory/{}'.format(category),\n",
    "        'Keypoints_Recall/AR@100 (medium) ByCategory/{}'.format(category)\n",
    "    ]\n",
    "\n",
    "    def first_value_func():\n",
    "      self._metrics = self.evaluate()\n",
    "      self.clear()\n",
    "      return np.float32(self._metrics[metric_names[0]])\n",
    "\n",
    "    def value_func_factory(metric_name):\n",
    "      def value_func():\n",
    "        return np.float32(self._metrics[metric_name])\n",
    "      return value_func\n",
    "\n",
    "    # Ensure that the metrics are only evaluated once.\n",
    "    first_value_op = tf.py_func(first_value_func, [], tf.float32)\n",
    "    eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}\n",
    "    with tf.control_dependencies([first_value_op]):\n",
    "      for metric_name in metric_names[1:]:\n",
    "        eval_metric_ops[metric_name] = (tf.py_func(\n",
    "            value_func_factory(metric_name), [], np.float32), update_op)\n",
    "    return eval_metric_ops\n",
    "\n",
    "\n",
    "class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):\n",
    "  \"\"\"Class to evaluate COCO detection metrics.\"\"\"\n",
    "\n",
    "  def __init__(self, categories,\n",
    "               include_metrics_per_category=False,\n",
    "               all_metrics_per_category=False,\n",
    "               super_categories=None):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      categories: A list of dicts, each of which has the following keys -\n",
    "        'id': (required) an integer id uniquely identifying this category.\n",
    "        'name': (required) string representing category name e.g., 'cat', 'dog'.\n",
    "      include_metrics_per_category: If True, include metrics for each category.\n",
    "      all_metrics_per_category: Whether to include all the summary metrics for\n",
    "        each category in per_category_ap. Be careful with setting it to true if\n",
    "        you have more than handful of categories, because it will pollute\n",
    "        your mldash.\n",
    "      super_categories: None or a python dict mapping super-category names\n",
    "        (strings) to lists of categories (corresponding to category names\n",
    "        in the label_map).  Metrics are aggregated along these super-categories\n",
    "        and added to the `per_category_ap` and are associated with the name\n",
    "          `PerformanceBySuperCategory/<super-category-name>`.\n",
    "    \"\"\"\n",
    "    super(CocoMaskEvaluator, self).__init__(categories)\n",
    "    self._image_id_to_mask_shape_map = {}\n",
    "    self._image_ids_with_detections = set([])\n",
    "    self._groundtruth_list = []\n",
    "    self._detection_masks_list = []\n",
    "    self._category_id_set = set([cat['id'] for cat in self._categories])\n",
    "    self._annotation_id = 1\n",
    "    self._include_metrics_per_category = include_metrics_per_category\n",
    "    self._super_categories = super_categories\n",
    "    self._all_metrics_per_category = all_metrics_per_category\n",
    "\n",
    "  def clear(self):\n",
    "    \"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"\n",
    "    self._image_id_to_mask_shape_map.clear()\n",
    "    self._image_ids_with_detections.clear()\n",
    "    self._groundtruth_list = []\n",
    "    self._detection_masks_list = []\n",
    "\n",
    "  def add_single_ground_truth_image_info(self,\n",
    "                                         image_id,\n",
    "                                         groundtruth_dict):\n",
    "    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n",
    "\n",
    "    If the image has already been added, a warning is logged, and groundtruth is\n",
    "    ignored.\n",
    "\n",
    "    Args:\n",
    "      image_id: A unique string/integer identifier for the image.\n",
    "      groundtruth_dict: A dictionary containing -\n",
    "        InputDataFields.groundtruth_boxes: float32 numpy array of shape\n",
    "          [num_boxes, 4] containing `num_boxes` groundtruth boxes of the format\n",
    "          [ymin, xmin, ymax, xmax] in absolute image coordinates.\n",
    "        InputDataFields.groundtruth_classes: integer numpy array of shape\n",
    "          [num_boxes] containing 1-indexed groundtruth classes for the boxes.\n",
    "        InputDataFields.groundtruth_instance_masks: uint8 numpy array of shape\n",
    "          [num_boxes, image_height, image_width] containing groundtruth masks\n",
    "          corresponding to the boxes. The elements of the array must be in\n",
    "          {0, 1}.\n",
    "        InputDataFields.groundtruth_is_crowd (optional): integer numpy array of\n",
    "          shape [num_boxes] containing iscrowd flag for groundtruth boxes.\n",
    "        InputDataFields.groundtruth_area (optional): float numpy array of\n",
    "          shape [num_boxes] containing the area (in the original absolute\n",
    "          coordinates) of the annotated object.\n",
    "    \"\"\"\n",
    "    if image_id in self._image_id_to_mask_shape_map:\n",
    "      tf.logging.warning('Ignoring ground truth with image id %s since it was '\n",
    "                         'previously added', image_id)\n",
    "      return\n",
    "\n",
    "    # Drop optional fields if empty tensor.\n",
    "    groundtruth_is_crowd = groundtruth_dict.get(\n",
    "        standard_fields.InputDataFields.groundtruth_is_crowd)\n",
    "    groundtruth_area = groundtruth_dict.get(\n",
    "        standard_fields.InputDataFields.groundtruth_area)\n",
    "    if groundtruth_is_crowd is not None and not groundtruth_is_crowd.shape[0]:\n",
    "      groundtruth_is_crowd = None\n",
    "    if groundtruth_area is not None and not groundtruth_area.shape[0]:\n",
    "      groundtruth_area = None\n",
    "\n",
    "    groundtruth_instance_masks = groundtruth_dict[\n",
    "        standard_fields.InputDataFields.groundtruth_instance_masks]\n",
    "    groundtruth_instance_masks = convert_masks_to_binary(\n",
    "        groundtruth_instance_masks)\n",
    "    self._groundtruth_list.extend(\n",
    "        coco_tools.\n",
    "        ExportSingleImageGroundtruthToCoco(\n",
    "            image_id=image_id,\n",
    "            next_annotation_id=self._annotation_id,\n",
    "            category_id_set=self._category_id_set,\n",
    "            groundtruth_boxes=groundtruth_dict[standard_fields.InputDataFields.\n",
    "                                               groundtruth_boxes],\n",
    "            groundtruth_classes=groundtruth_dict[standard_fields.\n",
    "                                                 InputDataFields.\n",
    "                                                 groundtruth_classes],\n",
    "            groundtruth_masks=groundtruth_instance_masks,\n",
    "            groundtruth_is_crowd=groundtruth_is_crowd,\n",
    "            groundtruth_area=groundtruth_area))\n",
    "    self._annotation_id += groundtruth_dict[standard_fields.InputDataFields.\n",
    "                                            groundtruth_boxes].shape[0]\n",
    "    self._image_id_to_mask_shape_map[image_id] = groundtruth_dict[\n",
    "        standard_fields.InputDataFields.groundtruth_instance_masks].shape\n",
    "\n",
    "  def add_single_detected_image_info(self,\n",
    "                                     image_id,\n",
    "                                     detections_dict):\n",
    "    \"\"\"Adds detections for a single image to be used for evaluation.\n",
    "\n",
    "    If a detection has already been added for this image id, a warning is\n",
    "    logged, and the detection is skipped.\n",
    "\n",
    "    Args:\n",
    "      image_id: A unique string/integer identifier for the image.\n",
    "      detections_dict: A dictionary containing -\n",
    "        DetectionResultFields.detection_scores: float32 numpy array of shape\n",
    "          [num_boxes] containing detection scores for the boxes.\n",
    "        DetectionResultFields.detection_classes: integer numpy array of shape\n",
    "          [num_boxes] containing 1-indexed detection classes for the boxes.\n",
    "        DetectionResultFields.detection_masks: optional uint8 numpy array of\n",
    "          shape [num_boxes, image_height, image_width] containing instance\n",
    "          masks corresponding to the boxes. The elements of the array must be\n",
    "          in {0, 1}.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If groundtruth for the image_id is not available or if\n",
    "        spatial shapes of groundtruth_instance_masks and detection_masks are\n",
    "        incompatible.\n",
    "    \"\"\"\n",
    "    if image_id not in self._image_id_to_mask_shape_map:\n",
    "      raise ValueError('Missing groundtruth for image id: {}'.format(image_id))\n",
    "\n",
    "    if image_id in self._image_ids_with_detections:\n",
    "      tf.logging.warning('Ignoring detection with image id %s since it was '\n",
    "                         'previously added', image_id)\n",
    "      return\n",
    "\n",
    "    groundtruth_masks_shape = self._image_id_to_mask_shape_map[image_id]\n",
    "    detection_masks = detections_dict[standard_fields.DetectionResultFields.\n",
    "                                      detection_masks]\n",
    "    if groundtruth_masks_shape[1:] != detection_masks.shape[1:]:\n",
    "      raise ValueError('Spatial shape of groundtruth masks and detection masks '\n",
    "                       'are incompatible: {} vs {}'.format(\n",
    "                           groundtruth_masks_shape,\n",
    "                           detection_masks.shape))\n",
    "    detection_masks = convert_masks_to_binary(detection_masks)\n",
    "    self._detection_masks_list.extend(\n",
    "        coco_tools.ExportSingleImageDetectionMasksToCoco(\n",
    "            image_id=image_id,\n",
    "            category_id_set=self._category_id_set,\n",
    "            detection_masks=detection_masks,\n",
    "            detection_scores=detections_dict[standard_fields.\n",
    "                                             DetectionResultFields.\n",
    "                                             detection_scores],\n",
    "            detection_classes=detections_dict[standard_fields.\n",
    "                                              DetectionResultFields.\n",
    "                                              detection_classes]))\n",
    "    self._image_ids_with_detections.update([image_id])\n",
    "\n",
    "  def dump_detections_to_json_file(self, json_output_path):\n",
    "    \"\"\"Saves the detections into json_output_path in the format used by MS COCO.\n",
    "\n",
    "    Args:\n",
    "      json_output_path: String containing the output file's path. It can be also\n",
    "        None. In that case nothing will be written to the output file.\n",
    "    \"\"\"\n",
    "    if json_output_path and json_output_path is not None:\n",
    "      tf.logging.info('Dumping detections to output json file.')\n",
    "      with tf.gfile.GFile(json_output_path, 'w') as fid:\n",
    "        json_utils.Dump(\n",
    "            obj=self._detection_masks_list, fid=fid, float_digits=4, indent=2)\n",
    "\n",
    "  def evaluate(self):\n",
    "    \"\"\"Evaluates the detection masks and returns a dictionary of coco metrics.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary holding -\n",
    "\n",
    "      1. summary_metrics:\n",
    "      'DetectionMasks_Precision/mAP': mean average precision over classes\n",
    "        averaged over IOU thresholds ranging from .5 to .95 with .05 increments.\n",
    "      'DetectionMasks_Precision/mAP@.50IOU': mean average precision at 50% IOU.\n",
    "      'DetectionMasks_Precision/mAP@.75IOU': mean average precision at 75% IOU.\n",
    "      'DetectionMasks_Precision/mAP (small)': mean average precision for small\n",
    "        objects (area < 32^2 pixels).\n",
    "      'DetectionMasks_Precision/mAP (medium)': mean average precision for medium\n",
    "        sized objects (32^2 pixels < area < 96^2 pixels).\n",
    "      'DetectionMasks_Precision/mAP (large)': mean average precision for large\n",
    "        objects (96^2 pixels < area < 10000^2 pixels).\n",
    "      'DetectionMasks_Recall/AR@1': average recall with 1 detection.\n",
    "      'DetectionMasks_Recall/AR@10': average recall with 10 detections.\n",
    "      'DetectionMasks_Recall/AR@100': average recall with 100 detections.\n",
    "      'DetectionMasks_Recall/AR@100 (small)': average recall for small objects\n",
    "        with 100 detections.\n",
    "      'DetectionMasks_Recall/AR@100 (medium)': average recall for medium objects\n",
    "        with 100 detections.\n",
    "      'DetectionMasks_Recall/AR@100 (large)': average recall for large objects\n",
    "        with 100 detections.\n",
    "\n",
    "      2. per_category_ap: if include_metrics_per_category is True, category\n",
    "      specific results with keys of the form:\n",
    "      'Precision mAP ByCategory/category' (without the supercategory part if\n",
    "      no supercategories exist). For backward compatibility\n",
    "      'PerformanceByCategory' is included in the output regardless of\n",
    "      all_metrics_per_category.\n",
    "        If super_categories are provided, then this will additionally include\n",
    "      metrics aggregated along the super_categories with keys of the form:\n",
    "      `PerformanceBySuperCategory/<super-category-name>`\n",
    "    \"\"\"\n",
    "    groundtruth_dict = {\n",
    "        'annotations': self._groundtruth_list,\n",
    "        'images': [{'id': image_id, 'height': shape[1], 'width': shape[2]}\n",
    "                   for image_id, shape in self._image_id_to_mask_shape_map.\n",
    "                   items()],\n",
    "        'categories': self._categories\n",
    "    }\n",
    "    coco_wrapped_groundtruth = coco_tools.COCOWrapper(\n",
    "        groundtruth_dict, detection_type='segmentation')\n",
    "    coco_wrapped_detection_masks = coco_wrapped_groundtruth.LoadAnnotations(\n",
    "        self._detection_masks_list)\n",
    "    mask_evaluator = coco_tools.COCOEvalWrapper(\n",
    "        coco_wrapped_groundtruth, coco_wrapped_detection_masks,\n",
    "        agnostic_mode=False, iou_type='segm')\n",
    "    mask_metrics, mask_per_category_ap = mask_evaluator.ComputeMetrics(\n",
    "        include_metrics_per_category=self._include_metrics_per_category,\n",
    "        super_categories=self._super_categories,\n",
    "        all_metrics_per_category=self._all_metrics_per_category)\n",
    "    mask_metrics.update(mask_per_category_ap)\n",
    "    mask_metrics = {'DetectionMasks_'+ key: value\n",
    "                    for key, value in mask_metrics.items()}\n",
    "    return mask_metrics\n",
    "\n",
    "  def add_eval_dict(self, eval_dict):\n",
    "    \"\"\"Observes an evaluation result dict for a single example.\n",
    "\n",
    "    When executing eagerly, once all observations have been observed by this\n",
    "    method you can use `.evaluate()` to get the final metrics.\n",
    "\n",
    "    When using `tf.estimator.Estimator` for evaluation this function is used by\n",
    "    `get_estimator_eval_metric_ops()` to construct the metric update op.\n",
    "\n",
    "    Args:\n",
    "      eval_dict: A dictionary that holds tensors for evaluating an object\n",
    "        detection model, returned from\n",
    "        eval_util.result_dict_for_single_example().\n",
    "\n",
    "    Returns:\n",
    "      None when executing eagerly, or an update_op that can be used to update\n",
    "      the eval metrics in `tf.estimator.EstimatorSpec`.\n",
    "    \"\"\"\n",
    "    def update_op(image_id_batched, groundtruth_boxes_batched,\n",
    "                  groundtruth_classes_batched,\n",
    "                  groundtruth_instance_masks_batched,\n",
    "                  groundtruth_is_crowd_batched, num_gt_boxes_per_image,\n",
    "                  detection_scores_batched, detection_classes_batched,\n",
    "                  detection_masks_batched, num_det_boxes_per_image,\n",
    "                  original_image_spatial_shape):\n",
    "      \"\"\"Update op for metrics.\"\"\"\n",
    "\n",
    "      for (image_id, groundtruth_boxes, groundtruth_classes,\n",
    "           groundtruth_instance_masks, groundtruth_is_crowd, num_gt_box,\n",
    "           detection_scores, detection_classes,\n",
    "           detection_masks, num_det_box, original_image_shape) in zip(\n",
    "               image_id_batched, groundtruth_boxes_batched,\n",
    "               groundtruth_classes_batched, groundtruth_instance_masks_batched,\n",
    "               groundtruth_is_crowd_batched, num_gt_boxes_per_image,\n",
    "               detection_scores_batched, detection_classes_batched,\n",
    "               detection_masks_batched, num_det_boxes_per_image,\n",
    "               original_image_spatial_shape):\n",
    "        self.add_single_ground_truth_image_info(\n",
    "            image_id, {\n",
    "                'groundtruth_boxes':\n",
    "                    groundtruth_boxes[:num_gt_box],\n",
    "                'groundtruth_classes':\n",
    "                    groundtruth_classes[:num_gt_box],\n",
    "                'groundtruth_instance_masks':\n",
    "                    groundtruth_instance_masks[\n",
    "                        :num_gt_box,\n",
    "                        :original_image_shape[0],\n",
    "                        :original_image_shape[1]],\n",
    "                'groundtruth_is_crowd':\n",
    "                    groundtruth_is_crowd[:num_gt_box]\n",
    "            })\n",
    "        self.add_single_detected_image_info(\n",
    "            image_id, {\n",
    "                'detection_scores': detection_scores[:num_det_box],\n",
    "                'detection_classes': detection_classes[:num_det_box],\n",
    "                'detection_masks': detection_masks[\n",
    "                    :num_det_box,\n",
    "                    :original_image_shape[0],\n",
    "                    :original_image_shape[1]]\n",
    "            })\n",
    "\n",
    "    # Unpack items from the evaluation dictionary.\n",
    "    input_data_fields = standard_fields.InputDataFields\n",
    "    detection_fields = standard_fields.DetectionResultFields\n",
    "    image_id = eval_dict[input_data_fields.key]\n",
    "    original_image_spatial_shape = eval_dict[\n",
    "        input_data_fields.original_image_spatial_shape]\n",
    "    groundtruth_boxes = eval_dict[input_data_fields.groundtruth_boxes]\n",
    "    groundtruth_classes = eval_dict[input_data_fields.groundtruth_classes]\n",
    "    groundtruth_instance_masks = eval_dict[\n",
    "        input_data_fields.groundtruth_instance_masks]\n",
    "    groundtruth_is_crowd = eval_dict.get(\n",
    "        input_data_fields.groundtruth_is_crowd, None)\n",
    "    num_gt_boxes_per_image = eval_dict.get(\n",
    "        input_data_fields.num_groundtruth_boxes, None)\n",
    "    detection_scores = eval_dict[detection_fields.detection_scores]\n",
    "    detection_classes = eval_dict[detection_fields.detection_classes]\n",
    "    detection_masks = eval_dict[detection_fields.detection_masks]\n",
    "    num_det_boxes_per_image = eval_dict.get(detection_fields.num_detections,\n",
    "                                            None)\n",
    "\n",
    "    if groundtruth_is_crowd is None:\n",
    "      groundtruth_is_crowd = tf.zeros_like(groundtruth_classes, dtype=tf.bool)\n",
    "\n",
    "    if not image_id.shape.as_list():\n",
    "      # Apply a batch dimension to all tensors.\n",
    "      image_id = tf.expand_dims(image_id, 0)\n",
    "      groundtruth_boxes = tf.expand_dims(groundtruth_boxes, 0)\n",
    "      groundtruth_classes = tf.expand_dims(groundtruth_classes, 0)\n",
    "      groundtruth_instance_masks = tf.expand_dims(groundtruth_instance_masks, 0)\n",
    "      groundtruth_is_crowd = tf.expand_dims(groundtruth_is_crowd, 0)\n",
    "      detection_scores = tf.expand_dims(detection_scores, 0)\n",
    "      detection_classes = tf.expand_dims(detection_classes, 0)\n",
    "      detection_masks = tf.expand_dims(detection_masks, 0)\n",
    "\n",
    "      if num_gt_boxes_per_image is None:\n",
    "        num_gt_boxes_per_image = tf.shape(groundtruth_boxes)[1:2]\n",
    "      else:\n",
    "        num_gt_boxes_per_image = tf.expand_dims(num_gt_boxes_per_image, 0)\n",
    "\n",
    "      if num_det_boxes_per_image is None:\n",
    "        num_det_boxes_per_image = tf.shape(detection_scores)[1:2]\n",
    "      else:\n",
    "        num_det_boxes_per_image = tf.expand_dims(num_det_boxes_per_image, 0)\n",
    "    else:\n",
    "      if num_gt_boxes_per_image is None:\n",
    "        num_gt_boxes_per_image = tf.tile(\n",
    "            tf.shape(groundtruth_boxes)[1:2],\n",
    "            multiples=tf.shape(groundtruth_boxes)[0:1])\n",
    "      if num_det_boxes_per_image is None:\n",
    "        num_det_boxes_per_image = tf.tile(\n",
    "            tf.shape(detection_scores)[1:2],\n",
    "            multiples=tf.shape(detection_scores)[0:1])\n",
    "\n",
    "    return tf.py_func(update_op, [\n",
    "        image_id, groundtruth_boxes, groundtruth_classes,\n",
    "        groundtruth_instance_masks, groundtruth_is_crowd,\n",
    "        num_gt_boxes_per_image, detection_scores, detection_classes,\n",
    "        detection_masks, num_det_boxes_per_image, original_image_spatial_shape\n",
    "    ], [])\n",
    "\n",
    "  def get_estimator_eval_metric_ops(self, eval_dict):\n",
    "    \"\"\"Returns a dictionary of eval metric ops.\n",
    "\n",
    "    Note that once value_op is called, the detections and groundtruth added via\n",
    "    update_op are cleared.\n",
    "\n",
    "    Args:\n",
    "      eval_dict: A dictionary that holds tensors for evaluating object detection\n",
    "        performance. For single-image evaluation, this dictionary may be\n",
    "        produced from eval_util.result_dict_for_single_example(). If multi-image\n",
    "        evaluation, `eval_dict` should contain the fields\n",
    "        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to\n",
    "        properly unpad the tensors from the batch.\n",
    "\n",
    "    Returns:\n",
    "      a dictionary of metric names to tuple of value_op and update_op that can\n",
    "      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all\n",
    "      update ops  must be run together and similarly all value ops must be run\n",
    "      together to guarantee correct behaviour.\n",
    "    \"\"\"\n",
    "    update_op = self.add_eval_dict(eval_dict)\n",
    "    metric_names = ['DetectionMasks_Precision/mAP',\n",
    "                    'DetectionMasks_Precision/mAP@.50IOU',\n",
    "                    'DetectionMasks_Precision/mAP@.75IOU',\n",
    "                    'DetectionMasks_Precision/mAP (small)',\n",
    "                    'DetectionMasks_Precision/mAP (medium)',\n",
    "                    'DetectionMasks_Precision/mAP (large)',\n",
    "                    'DetectionMasks_Recall/AR@1',\n",
    "                    'DetectionMasks_Recall/AR@10',\n",
    "                    'DetectionMasks_Recall/AR@100',\n",
    "                    'DetectionMasks_Recall/AR@100 (small)',\n",
    "                    'DetectionMasks_Recall/AR@100 (medium)',\n",
    "                    'DetectionMasks_Recall/AR@100 (large)']\n",
    "    if self._include_metrics_per_category:\n",
    "      for category_dict in self._categories:\n",
    "        metric_names.append('DetectionMasks_PerformanceByCategory/mAP/' +\n",
    "                            category_dict['name'])\n",
    "\n",
    "    def first_value_func():\n",
    "      self._metrics = self.evaluate()\n",
    "      self.clear()\n",
    "      return np.float32(self._metrics[metric_names[0]])\n",
    "\n",
    "    def value_func_factory(metric_name):\n",
    "      def value_func():\n",
    "        return np.float32(self._metrics[metric_name])\n",
    "      return value_func\n",
    "\n",
    "    # Ensure that the metrics are only evaluated once.\n",
    "    first_value_op = tf.py_func(first_value_func, [], tf.float32)\n",
    "    eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}\n",
    "    with tf.control_dependencies([first_value_op]):\n",
    "      for metric_name in metric_names[1:]:\n",
    "        eval_metric_ops[metric_name] = (tf.py_func(\n",
    "            value_func_factory(metric_name), [], np.float32), update_op)\n",
    "    return eval_metric_ops\n",
    "\n",
    "\n",
    "class CocoPanopticSegmentationEvaluator(\n",
    "    object_detection_evaluation.DetectionEvaluator):\n",
    "  \"\"\"Class to evaluate PQ (panoptic quality) metric on COCO dataset.\n",
    "\n",
    "  More details about this metric: https://arxiv.org/pdf/1801.00868.pdf.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               categories,\n",
    "               include_metrics_per_category=False,\n",
    "               iou_threshold=0.5,\n",
    "               ioa_threshold=0.5):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      categories: A list of dicts, each of which has the following keys -\n",
    "        'id': (required) an integer id uniquely identifying this category.\n",
    "        'name': (required) string representing category name e.g., 'cat', 'dog'.\n",
    "      include_metrics_per_category: If True, include metrics for each category.\n",
    "      iou_threshold: intersection-over-union threshold for mask matching (with\n",
    "        normal groundtruths).\n",
    "      ioa_threshold: intersection-over-area threshold for mask matching with\n",
    "        \"is_crowd\" groundtruths.\n",
    "    \"\"\"\n",
    "    super(CocoPanopticSegmentationEvaluator, self).__init__(categories)\n",
    "    self._groundtruth_masks = {}\n",
    "    self._groundtruth_class_labels = {}\n",
    "    self._groundtruth_is_crowd = {}\n",
    "    self._predicted_masks = {}\n",
    "    self._predicted_class_labels = {}\n",
    "    self._include_metrics_per_category = include_metrics_per_category\n",
    "    self._iou_threshold = iou_threshold\n",
    "    self._ioa_threshold = ioa_threshold\n",
    "\n",
    "  def clear(self):\n",
    "    \"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"\n",
    "    self._groundtruth_masks.clear()\n",
    "    self._groundtruth_class_labels.clear()\n",
    "    self._groundtruth_is_crowd.clear()\n",
    "    self._predicted_masks.clear()\n",
    "    self._predicted_class_labels.clear()\n",
    "\n",
    "  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n",
    "    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n",
    "\n",
    "    If the image has already been added, a warning is logged, and groundtruth is\n",
    "    ignored.\n",
    "\n",
    "    Args:\n",
    "      image_id: A unique string/integer identifier for the image.\n",
    "      groundtruth_dict: A dictionary containing -\n",
    "        InputDataFields.groundtruth_classes: integer numpy array of shape\n",
    "          [num_masks] containing 1-indexed groundtruth classes for the mask.\n",
    "        InputDataFields.groundtruth_instance_masks: uint8 numpy array of shape\n",
    "          [num_masks, image_height, image_width] containing groundtruth masks.\n",
    "          The elements of the array must be in {0, 1}.\n",
    "        InputDataFields.groundtruth_is_crowd (optional): integer numpy array of\n",
    "          shape [num_boxes] containing iscrowd flag for groundtruth boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    if image_id in self._groundtruth_masks:\n",
    "      tf.logging.warning(\n",
    "          'Ignoring groundtruth with image %s, since it has already been '\n",
    "          'added to the ground truth database.', image_id)\n",
    "      return\n",
    "\n",
    "    self._groundtruth_masks[image_id] = groundtruth_dict[\n",
    "        standard_fields.InputDataFields.groundtruth_instance_masks]\n",
    "    self._groundtruth_class_labels[image_id] = groundtruth_dict[\n",
    "        standard_fields.InputDataFields.groundtruth_classes]\n",
    "    groundtruth_is_crowd = groundtruth_dict.get(\n",
    "        standard_fields.InputDataFields.groundtruth_is_crowd)\n",
    "    # Drop groundtruth_is_crowd if empty tensor.\n",
    "    if groundtruth_is_crowd is not None and not groundtruth_is_crowd.size > 0:\n",
    "      groundtruth_is_crowd = None\n",
    "    if groundtruth_is_crowd is not None:\n",
    "      self._groundtruth_is_crowd[image_id] = groundtruth_is_crowd\n",
    "\n",
    "  def add_single_detected_image_info(self, image_id, detections_dict):\n",
    "    \"\"\"Adds detections for a single image to be used for evaluation.\n",
    "\n",
    "    If a detection has already been added for this image id, a warning is\n",
    "    logged, and the detection is skipped.\n",
    "\n",
    "    Args:\n",
    "      image_id: A unique string/integer identifier for the image.\n",
    "      detections_dict: A dictionary containing -\n",
    "        DetectionResultFields.detection_classes: integer numpy array of shape\n",
    "          [num_masks] containing 1-indexed detection classes for the masks.\n",
    "        DetectionResultFields.detection_masks: optional uint8 numpy array of\n",
    "          shape [num_masks, image_height, image_width] containing instance\n",
    "          masks. The elements of the array must be in {0, 1}.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If results and groundtruth shape don't match.\n",
    "    \"\"\"\n",
    "\n",
    "    if image_id not in self._groundtruth_masks:\n",
    "      raise ValueError('Missing groundtruth for image id: {}'.format(image_id))\n",
    "\n",
    "    detection_masks = detections_dict[\n",
    "        standard_fields.DetectionResultFields.detection_masks]\n",
    "    self._predicted_masks[image_id] = detection_masks\n",
    "    self._predicted_class_labels[image_id] = detections_dict[\n",
    "        standard_fields.DetectionResultFields.detection_classes]\n",
    "    groundtruth_mask_shape = self._groundtruth_masks[image_id].shape\n",
    "    if groundtruth_mask_shape[1:] != detection_masks.shape[1:]:\n",
    "      raise ValueError(\"The shape of results doesn't match groundtruth.\")\n",
    "\n",
    "  def evaluate(self):\n",
    "    \"\"\"Evaluates the detection masks and returns a dictionary of coco metrics.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary holding -\n",
    "\n",
    "      1. summary_metric:\n",
    "      'PanopticQuality@%.2fIOU': mean panoptic quality averaged over classes at\n",
    "        the required IOU.\n",
    "      'SegmentationQuality@%.2fIOU': mean segmentation quality averaged over\n",
    "        classes at the required IOU.\n",
    "      'RecognitionQuality@%.2fIOU': mean recognition quality averaged over\n",
    "        classes at the required IOU.\n",
    "      'NumValidClasses': number of valid classes. A valid class should have at\n",
    "        least one normal (is_crowd=0) groundtruth mask or one predicted mask.\n",
    "      'NumTotalClasses': number of total classes.\n",
    "\n",
    "      2. per_category_pq: if include_metrics_per_category is True, category\n",
    "      specific results with keys of the form:\n",
    "      'PanopticQuality@%.2fIOU_ByCategory/category'.\n",
    "    \"\"\"\n",
    "    # Evaluate and accumulate the iou/tp/fp/fn.\n",
    "    sum_tp_iou, sum_num_tp, sum_num_fp, sum_num_fn = self._evaluate_all_masks()\n",
    "    # Compute PQ metric for each category and average over all classes.\n",
    "    mask_metrics = self._compute_panoptic_metrics(sum_tp_iou, sum_num_tp,\n",
    "                                                  sum_num_fp, sum_num_fn)\n",
    "    return mask_metrics\n",
    "\n",
    "  def get_estimator_eval_metric_ops(self, eval_dict):\n",
    "    \"\"\"Returns a dictionary of eval metric ops.\n",
    "\n",
    "    Note that once value_op is called, the detections and groundtruth added via\n",
    "    update_op are cleared.\n",
    "\n",
    "    Args:\n",
    "      eval_dict: A dictionary that holds tensors for evaluating object detection\n",
    "        performance. For single-image evaluation, this dictionary may be\n",
    "        produced from eval_util.result_dict_for_single_example(). If multi-image\n",
    "        evaluation, `eval_dict` should contain the fields\n",
    "        'num_gt_masks_per_image' and 'num_det_masks_per_image' to properly unpad\n",
    "        the tensors from the batch.\n",
    "\n",
    "    Returns:\n",
    "      a dictionary of metric names to tuple of value_op and update_op that can\n",
    "      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all\n",
    "      update ops  must be run together and similarly all value ops must be run\n",
    "      together to guarantee correct behaviour.\n",
    "    \"\"\"\n",
    "\n",
    "    def update_op(image_id_batched, groundtruth_classes_batched,\n",
    "                  groundtruth_instance_masks_batched,\n",
    "                  groundtruth_is_crowd_batched, num_gt_masks_per_image,\n",
    "                  detection_classes_batched, detection_masks_batched,\n",
    "                  num_det_masks_per_image):\n",
    "      \"\"\"Update op for metrics.\"\"\"\n",
    "      for (image_id, groundtruth_classes, groundtruth_instance_masks,\n",
    "           groundtruth_is_crowd, num_gt_mask, detection_classes,\n",
    "           detection_masks, num_det_mask) in zip(\n",
    "               image_id_batched, groundtruth_classes_batched,\n",
    "               groundtruth_instance_masks_batched, groundtruth_is_crowd_batched,\n",
    "               num_gt_masks_per_image, detection_classes_batched,\n",
    "               detection_masks_batched, num_det_masks_per_image):\n",
    "\n",
    "        self.add_single_ground_truth_image_info(\n",
    "            image_id, {\n",
    "                'groundtruth_classes':\n",
    "                    groundtruth_classes[:num_gt_mask],\n",
    "                'groundtruth_instance_masks':\n",
    "                    groundtruth_instance_masks[:num_gt_mask],\n",
    "                'groundtruth_is_crowd':\n",
    "                    groundtruth_is_crowd[:num_gt_mask]\n",
    "            })\n",
    "        self.add_single_detected_image_info(\n",
    "            image_id, {\n",
    "                'detection_classes': detection_classes[:num_det_mask],\n",
    "                'detection_masks': detection_masks[:num_det_mask]\n",
    "            })\n",
    "\n",
    "    # Unpack items from the evaluation dictionary.\n",
    "    (image_id, groundtruth_classes, groundtruth_instance_masks,\n",
    "     groundtruth_is_crowd, num_gt_masks_per_image, detection_classes,\n",
    "     detection_masks, num_det_masks_per_image\n",
    "    ) = self._unpack_evaluation_dictionary_items(eval_dict)\n",
    "\n",
    "    update_op = tf.py_func(update_op, [\n",
    "        image_id, groundtruth_classes, groundtruth_instance_masks,\n",
    "        groundtruth_is_crowd, num_gt_masks_per_image, detection_classes,\n",
    "        detection_masks, num_det_masks_per_image\n",
    "    ], [])\n",
    "\n",
    "    metric_names = [\n",
    "        'PanopticQuality@%.2fIOU' % self._iou_threshold,\n",
    "        'SegmentationQuality@%.2fIOU' % self._iou_threshold,\n",
    "        'RecognitionQuality@%.2fIOU' % self._iou_threshold\n",
    "    ]\n",
    "    if self._include_metrics_per_category:\n",
    "      for category_dict in self._categories:\n",
    "        metric_names.append('PanopticQuality@%.2fIOU_ByCategory/%s' %\n",
    "                            (self._iou_threshold, category_dict['name']))\n",
    "\n",
    "    def first_value_func():\n",
    "      self._metrics = self.evaluate()\n",
    "      self.clear()\n",
    "      return np.float32(self._metrics[metric_names[0]])\n",
    "\n",
    "    def value_func_factory(metric_name):\n",
    "\n",
    "      def value_func():\n",
    "        return np.float32(self._metrics[metric_name])\n",
    "\n",
    "      return value_func\n",
    "\n",
    "    # Ensure that the metrics are only evaluated once.\n",
    "    first_value_op = tf.py_func(first_value_func, [], tf.float32)\n",
    "    eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}\n",
    "    with tf.control_dependencies([first_value_op]):\n",
    "      for metric_name in metric_names[1:]:\n",
    "        eval_metric_ops[metric_name] = (tf.py_func(\n",
    "            value_func_factory(metric_name), [], np.float32), update_op)\n",
    "    return eval_metric_ops\n",
    "\n",
    "  def _evaluate_all_masks(self):\n",
    "    \"\"\"Evaluate all masks and compute sum iou/TP/FP/FN.\"\"\"\n",
    "\n",
    "    sum_num_tp = {category['id']: 0 for category in self._categories}\n",
    "    sum_num_fp = sum_num_tp.copy()\n",
    "    sum_num_fn = sum_num_tp.copy()\n",
    "    sum_tp_iou = sum_num_tp.copy()\n",
    "\n",
    "    for image_id in self._groundtruth_class_labels:\n",
    "      # Separate normal and is_crowd groundtruth\n",
    "      crowd_gt_indices = self._groundtruth_is_crowd.get(image_id)\n",
    "      (normal_gt_masks, normal_gt_classes, crowd_gt_masks,\n",
    "       crowd_gt_classes) = self._separate_normal_and_crowd_labels(\n",
    "           crowd_gt_indices, self._groundtruth_masks[image_id],\n",
    "           self._groundtruth_class_labels[image_id])\n",
    "\n",
    "      # Mask matching to normal GT.\n",
    "      predicted_masks = self._predicted_masks[image_id]\n",
    "      predicted_class_labels = self._predicted_class_labels[image_id]\n",
    "      (overlaps, pred_matched,\n",
    "       gt_matched) = self._match_predictions_to_groundtruths(\n",
    "           predicted_masks,\n",
    "           predicted_class_labels,\n",
    "           normal_gt_masks,\n",
    "           normal_gt_classes,\n",
    "           self._iou_threshold,\n",
    "           is_crowd=False,\n",
    "           with_replacement=False)\n",
    "\n",
    "      # Accumulate true positives.\n",
    "      for (class_id, is_matched, overlap) in zip(predicted_class_labels,\n",
    "                                                 pred_matched, overlaps):\n",
    "        if is_matched:\n",
    "          sum_num_tp[class_id] += 1\n",
    "          sum_tp_iou[class_id] += overlap\n",
    "\n",
    "      # Accumulate false negatives.\n",
    "      for (class_id, is_matched) in zip(normal_gt_classes, gt_matched):\n",
    "        if not is_matched:\n",
    "          sum_num_fn[class_id] += 1\n",
    "\n",
    "      # Match remaining predictions to crowd gt.\n",
    "      remained_pred_indices = np.logical_not(pred_matched)\n",
    "      remained_pred_masks = predicted_masks[remained_pred_indices, :, :]\n",
    "      remained_pred_classes = predicted_class_labels[remained_pred_indices]\n",
    "      _, pred_matched, _ = self._match_predictions_to_groundtruths(\n",
    "          remained_pred_masks,\n",
    "          remained_pred_classes,\n",
    "          crowd_gt_masks,\n",
    "          crowd_gt_classes,\n",
    "          self._ioa_threshold,\n",
    "          is_crowd=True,\n",
    "          with_replacement=True)\n",
    "\n",
    "      # Accumulate false positives\n",
    "      for (class_id, is_matched) in zip(remained_pred_classes, pred_matched):\n",
    "        if not is_matched:\n",
    "          sum_num_fp[class_id] += 1\n",
    "    return sum_tp_iou, sum_num_tp, sum_num_fp, sum_num_fn\n",
    "\n",
    "  def _compute_panoptic_metrics(self, sum_tp_iou, sum_num_tp, sum_num_fp,\n",
    "                                sum_num_fn):\n",
    "    \"\"\"Compute PQ metric for each category and average over all classes.\n",
    "\n",
    "    Args:\n",
    "      sum_tp_iou: dict, summed true positive intersection-over-union (IoU) for\n",
    "        each class, keyed by class_id.\n",
    "      sum_num_tp: the total number of true positives for each class, keyed by\n",
    "        class_id.\n",
    "      sum_num_fp: the total number of false positives for each class, keyed by\n",
    "        class_id.\n",
    "      sum_num_fn: the total number of false negatives for each class, keyed by\n",
    "        class_id.\n",
    "\n",
    "    Returns:\n",
    "      mask_metrics: a dictionary containing averaged metrics over all classes,\n",
    "        and per-category metrics if required.\n",
    "    \"\"\"\n",
    "    mask_metrics = {}\n",
    "    sum_pq = 0\n",
    "    sum_sq = 0\n",
    "    sum_rq = 0\n",
    "    num_valid_classes = 0\n",
    "    for category in self._categories:\n",
    "      class_id = category['id']\n",
    "      (panoptic_quality, segmentation_quality,\n",
    "       recognition_quality) = self._compute_panoptic_metrics_single_class(\n",
    "           sum_tp_iou[class_id], sum_num_tp[class_id], sum_num_fp[class_id],\n",
    "           sum_num_fn[class_id])\n",
    "      if panoptic_quality is not None:\n",
    "        sum_pq += panoptic_quality\n",
    "        sum_sq += segmentation_quality\n",
    "        sum_rq += recognition_quality\n",
    "        num_valid_classes += 1\n",
    "        if self._include_metrics_per_category:\n",
    "          mask_metrics['PanopticQuality@%.2fIOU_ByCategory/%s' %\n",
    "                       (self._iou_threshold,\n",
    "                        category['name'])] = panoptic_quality\n",
    "    mask_metrics['PanopticQuality@%.2fIOU' %\n",
    "                 self._iou_threshold] = sum_pq / num_valid_classes\n",
    "    mask_metrics['SegmentationQuality@%.2fIOU' %\n",
    "                 self._iou_threshold] = sum_sq / num_valid_classes\n",
    "    mask_metrics['RecognitionQuality@%.2fIOU' %\n",
    "                 self._iou_threshold] = sum_rq / num_valid_classes\n",
    "    mask_metrics['NumValidClasses'] = num_valid_classes\n",
    "    mask_metrics['NumTotalClasses'] = len(self._categories)\n",
    "    return mask_metrics\n",
    "\n",
    "  def _compute_panoptic_metrics_single_class(self, sum_tp_iou, num_tp, num_fp,\n",
    "                                             num_fn):\n",
    "    \"\"\"Compute panoptic metrics: panoptic/segmentation/recognition quality.\n",
    "\n",
    "    More computation details in https://arxiv.org/pdf/1801.00868.pdf.\n",
    "    Args:\n",
    "      sum_tp_iou: summed true positive intersection-over-union (IoU) for a\n",
    "        specific class.\n",
    "      num_tp: the total number of true positives for a specific class.\n",
    "      num_fp: the total number of false positives for a specific class.\n",
    "      num_fn: the total number of false negatives for a specific class.\n",
    "\n",
    "    Returns:\n",
    "      panoptic_quality: sum_tp_iou / (num_tp + 0.5*num_fp + 0.5*num_fn).\n",
    "      segmentation_quality: sum_tp_iou / num_tp.\n",
    "      recognition_quality: num_tp / (num_tp + 0.5*num_fp + 0.5*num_fn).\n",
    "    \"\"\"\n",
    "    denominator = num_tp + 0.5 * num_fp + 0.5 * num_fn\n",
    "    # Calculate metric only if there is at least one GT or one prediction.\n",
    "    if denominator > 0:\n",
    "      recognition_quality = num_tp / denominator\n",
    "      if num_tp > 0:\n",
    "        segmentation_quality = sum_tp_iou / num_tp\n",
    "      else:\n",
    "        # If there is no TP for this category.\n",
    "        segmentation_quality = 0\n",
    "      panoptic_quality = segmentation_quality * recognition_quality\n",
    "      return panoptic_quality, segmentation_quality, recognition_quality\n",
    "    else:\n",
    "      return None, None, None\n",
    "\n",
    "  def _separate_normal_and_crowd_labels(self, crowd_gt_indices,\n",
    "                                        groundtruth_masks, groundtruth_classes):\n",
    "    \"\"\"Separate normal and crowd groundtruth class_labels and masks.\n",
    "\n",
    "    Args:\n",
    "      crowd_gt_indices: None or array of shape [num_groundtruths]. If None, all\n",
    "        groundtruths are treated as normal ones.\n",
    "      groundtruth_masks: array of shape [num_groundtruths, height, width].\n",
    "      groundtruth_classes: array of shape [num_groundtruths].\n",
    "\n",
    "    Returns:\n",
    "      normal_gt_masks: array of shape [num_normal_groundtruths, height, width].\n",
    "      normal_gt_classes: array of shape [num_normal_groundtruths].\n",
    "      crowd_gt_masks: array of shape [num_crowd_groundtruths, height, width].\n",
    "      crowd_gt_classes: array of shape [num_crowd_groundtruths].\n",
    "    Raises:\n",
    "      ValueError: if the shape of groundtruth classes doesn't match groundtruth\n",
    "        masks or if the shape of crowd_gt_indices.\n",
    "    \"\"\"\n",
    "    if groundtruth_masks.shape[0] != groundtruth_classes.shape[0]:\n",
    "      raise ValueError(\n",
    "          \"The number of masks doesn't match the number of labels.\")\n",
    "    if crowd_gt_indices is None:\n",
    "      # All gts are treated as normal\n",
    "      crowd_gt_indices = np.zeros(groundtruth_masks.shape, dtype=np.bool)\n",
    "    else:\n",
    "      if groundtruth_masks.shape[0] != crowd_gt_indices.shape[0]:\n",
    "        raise ValueError(\n",
    "            \"The number of masks doesn't match the number of is_crowd labels.\")\n",
    "      crowd_gt_indices = crowd_gt_indices.astype(np.bool)\n",
    "    normal_gt_indices = np.logical_not(crowd_gt_indices)\n",
    "    if normal_gt_indices.size:\n",
    "      normal_gt_masks = groundtruth_masks[normal_gt_indices, :, :]\n",
    "      normal_gt_classes = groundtruth_classes[normal_gt_indices]\n",
    "      crowd_gt_masks = groundtruth_masks[crowd_gt_indices, :, :]\n",
    "      crowd_gt_classes = groundtruth_classes[crowd_gt_indices]\n",
    "    else:\n",
    "      # No groundtruths available, groundtruth_masks.shape = (0, h, w)\n",
    "      normal_gt_masks = groundtruth_masks\n",
    "      normal_gt_classes = groundtruth_classes\n",
    "      crowd_gt_masks = groundtruth_masks\n",
    "      crowd_gt_classes = groundtruth_classes\n",
    "    return normal_gt_masks, normal_gt_classes, crowd_gt_masks, crowd_gt_classes\n",
    "\n",
    "  def _match_predictions_to_groundtruths(self,\n",
    "                                         predicted_masks,\n",
    "                                         predicted_classes,\n",
    "                                         groundtruth_masks,\n",
    "                                         groundtruth_classes,\n",
    "                                         matching_threshold,\n",
    "                                         is_crowd=False,\n",
    "                                         with_replacement=False):\n",
    "    \"\"\"Match the predicted masks to groundtruths.\n",
    "\n",
    "    Args:\n",
    "      predicted_masks: array of shape [num_predictions, height, width].\n",
    "      predicted_classes: array of shape [num_predictions].\n",
    "      groundtruth_masks: array of shape [num_groundtruths, height, width].\n",
    "      groundtruth_classes: array of shape [num_groundtruths].\n",
    "      matching_threshold: if the overlap between a prediction and a groundtruth\n",
    "        is larger than this threshold, the prediction is true positive.\n",
    "      is_crowd: whether the groundtruths are crowd annotation or not. If True,\n",
    "        use intersection over area (IoA) as the overlapping metric; otherwise\n",
    "        use intersection over union (IoU).\n",
    "      with_replacement: whether a groundtruth can be matched to multiple\n",
    "        predictions. By default, for normal groundtruths, only 1-1 matching is\n",
    "        allowed for normal groundtruths; for crowd groundtruths, 1-to-many must\n",
    "        be allowed.\n",
    "\n",
    "    Returns:\n",
    "      best_overlaps: array of shape [num_predictions]. Values representing the\n",
    "      IoU\n",
    "        or IoA with best matched groundtruth.\n",
    "      pred_matched: array of shape [num_predictions]. Boolean value representing\n",
    "        whether the ith prediction is matched to a groundtruth.\n",
    "      gt_matched: array of shape [num_groundtruth]. Boolean value representing\n",
    "        whether the ith groundtruth is matched to a prediction.\n",
    "    Raises:\n",
    "      ValueError: if the shape of groundtruth/predicted masks doesn't match\n",
    "        groundtruth/predicted classes.\n",
    "    \"\"\"\n",
    "    if groundtruth_masks.shape[0] != groundtruth_classes.shape[0]:\n",
    "      raise ValueError(\n",
    "          \"The number of GT masks doesn't match the number of labels.\")\n",
    "    if predicted_masks.shape[0] != predicted_classes.shape[0]:\n",
    "      raise ValueError(\n",
    "          \"The number of predicted masks doesn't match the number of labels.\")\n",
    "    gt_matched = np.zeros(groundtruth_classes.shape, dtype=np.bool)\n",
    "    pred_matched = np.zeros(predicted_classes.shape, dtype=np.bool)\n",
    "    best_overlaps = np.zeros(predicted_classes.shape)\n",
    "    for pid in range(predicted_classes.shape[0]):\n",
    "      best_overlap = 0\n",
    "      matched_gt_id = -1\n",
    "      for gid in range(groundtruth_classes.shape[0]):\n",
    "        if predicted_classes[pid] == groundtruth_classes[gid]:\n",
    "          if (not with_replacement) and gt_matched[gid]:\n",
    "            continue\n",
    "          if not is_crowd:\n",
    "            overlap = np_mask_ops.iou(predicted_masks[pid:pid + 1],\n",
    "                                      groundtruth_masks[gid:gid + 1])[0, 0]\n",
    "          else:\n",
    "            overlap = np_mask_ops.ioa(groundtruth_masks[gid:gid + 1],\n",
    "                                      predicted_masks[pid:pid + 1])[0, 0]\n",
    "          if overlap >= matching_threshold and overlap > best_overlap:\n",
    "            matched_gt_id = gid\n",
    "            best_overlap = overlap\n",
    "      if matched_gt_id >= 0:\n",
    "        gt_matched[matched_gt_id] = True\n",
    "        pred_matched[pid] = True\n",
    "        best_overlaps[pid] = best_overlap\n",
    "    return best_overlaps, pred_matched, gt_matched\n",
    "\n",
    "  def _unpack_evaluation_dictionary_items(self, eval_dict):\n",
    "    \"\"\"Unpack items from the evaluation dictionary.\"\"\"\n",
    "    input_data_fields = standard_fields.InputDataFields\n",
    "    detection_fields = standard_fields.DetectionResultFields\n",
    "    image_id = eval_dict[input_data_fields.key]\n",
    "    groundtruth_classes = eval_dict[input_data_fields.groundtruth_classes]\n",
    "    groundtruth_instance_masks = eval_dict[\n",
    "        input_data_fields.groundtruth_instance_masks]\n",
    "    groundtruth_is_crowd = eval_dict.get(input_data_fields.groundtruth_is_crowd,\n",
    "                                         None)\n",
    "    num_gt_masks_per_image = eval_dict.get(\n",
    "        input_data_fields.num_groundtruth_boxes, None)\n",
    "    detection_classes = eval_dict[detection_fields.detection_classes]\n",
    "    detection_masks = eval_dict[detection_fields.detection_masks]\n",
    "    num_det_masks_per_image = eval_dict.get(detection_fields.num_detections,\n",
    "                                            None)\n",
    "    if groundtruth_is_crowd is None:\n",
    "      groundtruth_is_crowd = tf.zeros_like(groundtruth_classes, dtype=tf.bool)\n",
    "\n",
    "    if not image_id.shape.as_list():\n",
    "      # Apply a batch dimension to all tensors.\n",
    "      image_id = tf.expand_dims(image_id, 0)\n",
    "      groundtruth_classes = tf.expand_dims(groundtruth_classes, 0)\n",
    "      groundtruth_instance_masks = tf.expand_dims(groundtruth_instance_masks, 0)\n",
    "      groundtruth_is_crowd = tf.expand_dims(groundtruth_is_crowd, 0)\n",
    "      detection_classes = tf.expand_dims(detection_classes, 0)\n",
    "      detection_masks = tf.expand_dims(detection_masks, 0)\n",
    "\n",
    "      if num_gt_masks_per_image is None:\n",
    "        num_gt_masks_per_image = tf.shape(groundtruth_classes)[1:2]\n",
    "      else:\n",
    "        num_gt_masks_per_image = tf.expand_dims(num_gt_masks_per_image, 0)\n",
    "\n",
    "      if num_det_masks_per_image is None:\n",
    "        num_det_masks_per_image = tf.shape(detection_classes)[1:2]\n",
    "      else:\n",
    "        num_det_masks_per_image = tf.expand_dims(num_det_masks_per_image, 0)\n",
    "    else:\n",
    "      if num_gt_masks_per_image is None:\n",
    "        num_gt_masks_per_image = tf.tile(\n",
    "            tf.shape(groundtruth_classes)[1:2],\n",
    "            multiples=tf.shape(groundtruth_classes)[0:1])\n",
    "      if num_det_masks_per_image is None:\n",
    "        num_det_masks_per_image = tf.tile(\n",
    "            tf.shape(detection_classes)[1:2],\n",
    "            multiples=tf.shape(detection_classes)[0:1])\n",
    "    return (image_id, groundtruth_classes, groundtruth_instance_masks,\n",
    "            groundtruth_is_crowd, num_gt_masks_per_image, detection_classes,\n",
    "            detection_masks, num_det_masks_per_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7758802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
