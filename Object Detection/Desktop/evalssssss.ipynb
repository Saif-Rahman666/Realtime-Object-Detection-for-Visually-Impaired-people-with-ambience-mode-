{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f18c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d2a6a33947ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeypoint_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstandard_fields\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoco_evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlvis_evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meval_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\raihan\\anaconda3\\envs\\obj_detection\\lib\\site-packages\\object_detection\\metrics\\coco_evaluation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstandard_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoco_tools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mobject_detection_evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\raihan\\anaconda3\\envs\\obj_detection\\lib\\site-packages\\object_detection\\metrics\\coco_tools.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcocoeval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Common utility functions for evaluation.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import tf_slim as slim\n",
    "\n",
    "from object_detection.core import box_list\n",
    "from object_detection.core import box_list_ops\n",
    "from object_detection.core import keypoint_ops\n",
    "from object_detection.core import standard_fields as fields\n",
    "from object_detection.metrics import coco_evaluation\n",
    "from object_detection.metrics import lvis_evaluation\n",
    "from object_detection.protos import eval_pb2\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import object_detection_evaluation\n",
    "from object_detection.utils import ops\n",
    "from object_detection.utils import shape_utils\n",
    "from object_detection.utils import visualization_utils as vis_utils\n",
    "\n",
    "EVAL_KEYPOINT_METRIC = 'coco_keypoint_metrics'\n",
    "\n",
    "# A dictionary of metric names to classes that implement the metric. The classes\n",
    "# in the dictionary must implement\n",
    "# utils.object_detection_evaluation.DetectionEvaluator interface.\n",
    "EVAL_METRICS_CLASS_DICT = {\n",
    "    'coco_detection_metrics':\n",
    "        coco_evaluation.CocoDetectionEvaluator,\n",
    "    'coco_keypoint_metrics':\n",
    "        coco_evaluation.CocoKeypointEvaluator,\n",
    "    'coco_mask_metrics':\n",
    "        coco_evaluation.CocoMaskEvaluator,\n",
    "    'coco_panoptic_metrics':\n",
    "        coco_evaluation.CocoPanopticSegmentationEvaluator,\n",
    "    'lvis_mask_metrics':\n",
    "        lvis_evaluation.LVISMaskEvaluator,\n",
    "    'oid_challenge_detection_metrics':\n",
    "        object_detection_evaluation.OpenImagesDetectionChallengeEvaluator,\n",
    "    'oid_challenge_segmentation_metrics':\n",
    "        object_detection_evaluation\n",
    "        .OpenImagesInstanceSegmentationChallengeEvaluator,\n",
    "    'pascal_voc_detection_metrics':\n",
    "        object_detection_evaluation.PascalDetectionEvaluator,\n",
    "    'weighted_pascal_voc_detection_metrics':\n",
    "        object_detection_evaluation.WeightedPascalDetectionEvaluator,\n",
    "    'precision_at_recall_detection_metrics':\n",
    "        object_detection_evaluation.PrecisionAtRecallDetectionEvaluator,\n",
    "    'pascal_voc_instance_segmentation_metrics':\n",
    "        object_detection_evaluation.PascalInstanceSegmentationEvaluator,\n",
    "    'weighted_pascal_voc_instance_segmentation_metrics':\n",
    "        object_detection_evaluation.WeightedPascalInstanceSegmentationEvaluator,\n",
    "    'oid_V2_detection_metrics':\n",
    "        object_detection_evaluation.OpenImagesDetectionEvaluator,\n",
    "}\n",
    "\n",
    "EVAL_DEFAULT_METRIC = 'coco_detection_metrics'\n",
    "\n",
    "\n",
    "def write_metrics(metrics, global_step, summary_dir):\n",
    "  \"\"\"Write metrics to a summary directory.\n",
    "\n",
    "  Args:\n",
    "    metrics: A dictionary containing metric names and values.\n",
    "    global_step: Global step at which the metrics are computed.\n",
    "    summary_dir: Directory to write tensorflow summaries to.\n",
    "  \"\"\"\n",
    "  tf.logging.info('Writing metrics to tf summary.')\n",
    "  summary_writer = tf.summary.FileWriterCache.get(summary_dir)\n",
    "  for key in sorted(metrics):\n",
    "    summary = tf.Summary(value=[\n",
    "        tf.Summary.Value(tag=key, simple_value=metrics[key]),\n",
    "    ])\n",
    "    summary_writer.add_summary(summary, global_step)\n",
    "    tf.logging.info('%s: %f', key, metrics[key])\n",
    "  tf.logging.info('Metrics written to tf summary.')\n",
    "\n",
    "\n",
    "# TODO(rathodv): Add tests.\n",
    "def visualize_detection_results(result_dict,\n",
    "                                tag,\n",
    "                                global_step,\n",
    "                                categories,\n",
    "                                summary_dir='',\n",
    "                                export_dir='',\n",
    "                                agnostic_mode=False,\n",
    "                                show_groundtruth=False,\n",
    "                                groundtruth_box_visualization_color='black',\n",
    "                                min_score_thresh=.5,\n",
    "                                max_num_predictions=20,\n",
    "                                skip_scores=False,\n",
    "                                skip_labels=False,\n",
    "                                keep_image_id_for_visualization_export=False):\n",
    "  \"\"\"Visualizes detection results and writes visualizations to image summaries.\n",
    "\n",
    "  This function visualizes an image with its detected bounding boxes and writes\n",
    "  to image summaries which can be viewed on tensorboard.  It optionally also\n",
    "  writes images to a directory. In the case of missing entry in the label map,\n",
    "  unknown class name in the visualization is shown as \"N/A\".\n",
    "\n",
    "  Args:\n",
    "    result_dict: a dictionary holding groundtruth and detection\n",
    "      data corresponding to each image being evaluated.  The following keys\n",
    "      are required:\n",
    "        'original_image': a numpy array representing the image with shape\n",
    "          [1, height, width, 3] or [1, height, width, 1]\n",
    "        'detection_boxes': a numpy array of shape [N, 4]\n",
    "        'detection_scores': a numpy array of shape [N]\n",
    "        'detection_classes': a numpy array of shape [N]\n",
    "      The following keys are optional:\n",
    "        'groundtruth_boxes': a numpy array of shape [N, 4]\n",
    "        'groundtruth_keypoints': a numpy array of shape [N, num_keypoints, 2]\n",
    "      Detections are assumed to be provided in decreasing order of score and for\n",
    "      display, and we assume that scores are probabilities between 0 and 1.\n",
    "    tag: tensorboard tag (string) to associate with image.\n",
    "    global_step: global step at which the visualization are generated.\n",
    "    categories: a list of dictionaries representing all possible categories.\n",
    "      Each dict in this list has the following keys:\n",
    "          'id': (required) an integer id uniquely identifying this category\n",
    "          'name': (required) string representing category name\n",
    "            e.g., 'cat', 'dog', 'pizza'\n",
    "          'supercategory': (optional) string representing the supercategory\n",
    "            e.g., 'animal', 'vehicle', 'food', etc\n",
    "    summary_dir: the output directory to which the image summaries are written.\n",
    "    export_dir: the output directory to which images are written.  If this is\n",
    "      empty (default), then images are not exported.\n",
    "    agnostic_mode: boolean (default: False) controlling whether to evaluate in\n",
    "      class-agnostic mode or not.\n",
    "    show_groundtruth: boolean (default: False) controlling whether to show\n",
    "      groundtruth boxes in addition to detected boxes\n",
    "    groundtruth_box_visualization_color: box color for visualizing groundtruth\n",
    "      boxes\n",
    "    min_score_thresh: minimum score threshold for a box to be visualized\n",
    "    max_num_predictions: maximum number of detections to visualize\n",
    "    skip_scores: whether to skip score when drawing a single detection\n",
    "    skip_labels: whether to skip label when drawing a single detection\n",
    "    keep_image_id_for_visualization_export: whether to keep image identifier in\n",
    "      filename when exported to export_dir\n",
    "  Raises:\n",
    "    ValueError: if result_dict does not contain the expected keys (i.e.,\n",
    "      'original_image', 'detection_boxes', 'detection_scores',\n",
    "      'detection_classes')\n",
    "  \"\"\"\n",
    "  detection_fields = fields.DetectionResultFields\n",
    "  input_fields = fields.InputDataFields\n",
    "  if not set([\n",
    "      input_fields.original_image,\n",
    "      detection_fields.detection_boxes,\n",
    "      detection_fields.detection_scores,\n",
    "      detection_fields.detection_classes,\n",
    "  ]).issubset(set(result_dict.keys())):\n",
    "    raise ValueError('result_dict does not contain all expected keys.')\n",
    "  if show_groundtruth and input_fields.groundtruth_boxes not in result_dict:\n",
    "    raise ValueError('If show_groundtruth is enabled, result_dict must contain '\n",
    "                     'groundtruth_boxes.')\n",
    "  tf.logging.info('Creating detection visualizations.')\n",
    "  category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "  image = np.squeeze(result_dict[input_fields.original_image], axis=0)\n",
    "  if image.shape[2] == 1:  # If one channel image, repeat in RGB.\n",
    "    image = np.tile(image, [1, 1, 3])\n",
    "  detection_boxes = result_dict[detection_fields.detection_boxes]\n",
    "  detection_scores = result_dict[detection_fields.detection_scores]\n",
    "  detection_classes = np.int32((result_dict[\n",
    "      detection_fields.detection_classes]))\n",
    "  detection_keypoints = result_dict.get(detection_fields.detection_keypoints)\n",
    "  detection_masks = result_dict.get(detection_fields.detection_masks)\n",
    "  detection_boundaries = result_dict.get(detection_fields.detection_boundaries)\n",
    "\n",
    "  # Plot groundtruth underneath detections\n",
    "  if show_groundtruth:\n",
    "    groundtruth_boxes = result_dict[input_fields.groundtruth_boxes]\n",
    "    groundtruth_keypoints = result_dict.get(input_fields.groundtruth_keypoints)\n",
    "    vis_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image=image,\n",
    "        boxes=groundtruth_boxes,\n",
    "        classes=None,\n",
    "        scores=None,\n",
    "        category_index=category_index,\n",
    "        keypoints=groundtruth_keypoints,\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=None,\n",
    "        groundtruth_box_visualization_color=groundtruth_box_visualization_color)\n",
    "  vis_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image,\n",
    "      detection_boxes,\n",
    "      detection_classes,\n",
    "      detection_scores,\n",
    "      category_index,\n",
    "      instance_masks=detection_masks,\n",
    "      instance_boundaries=detection_boundaries,\n",
    "      keypoints=detection_keypoints,\n",
    "      use_normalized_coordinates=False,\n",
    "      max_boxes_to_draw=max_num_predictions,\n",
    "      min_score_thresh=min_score_thresh,\n",
    "      agnostic_mode=agnostic_mode,\n",
    "      skip_scores=skip_scores,\n",
    "      skip_labels=skip_labels)\n",
    "\n",
    "  if export_dir:\n",
    "    if keep_image_id_for_visualization_export and result_dict[fields.\n",
    "                                                              InputDataFields()\n",
    "                                                              .key]:\n",
    "      export_path = os.path.join(export_dir, 'export-{}-{}.png'.format(\n",
    "          tag, result_dict[fields.InputDataFields().key]))\n",
    "    else:\n",
    "      export_path = os.path.join(export_dir, 'export-{}.png'.format(tag))\n",
    "    vis_utils.save_image_array_as_png(image, export_path)\n",
    "\n",
    "  summary = tf.Summary(value=[\n",
    "      tf.Summary.Value(\n",
    "          tag=tag,\n",
    "          image=tf.Summary.Image(\n",
    "              encoded_image_string=vis_utils.encode_image_array_as_png_str(\n",
    "                  image)))\n",
    "  ])\n",
    "  summary_writer = tf.summary.FileWriterCache.get(summary_dir)\n",
    "  summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "  tf.logging.info('Detection visualizations written to summary with tag %s.',\n",
    "                  tag)\n",
    "\n",
    "\n",
    "def _run_checkpoint_once(tensor_dict,\n",
    "                         evaluators=None,\n",
    "                         batch_processor=None,\n",
    "                         checkpoint_dirs=None,\n",
    "                         variables_to_restore=None,\n",
    "                         restore_fn=None,\n",
    "                         num_batches=1,\n",
    "                         master='',\n",
    "                         save_graph=False,\n",
    "                         save_graph_dir='',\n",
    "                         losses_dict=None,\n",
    "                         eval_export_path=None,\n",
    "                         process_metrics_fn=None):\n",
    "  \"\"\"Evaluates metrics defined in evaluators and returns summaries.\n",
    "\n",
    "  This function loads the latest checkpoint in checkpoint_dirs and evaluates\n",
    "  all metrics defined in evaluators. The metrics are processed in batch by the\n",
    "  batch_processor.\n",
    "\n",
    "  Args:\n",
    "    tensor_dict: a dictionary holding tensors representing a batch of detections\n",
    "      and corresponding groundtruth annotations.\n",
    "    evaluators: a list of object of type DetectionEvaluator to be used for\n",
    "      evaluation. Note that the metric names produced by different evaluators\n",
    "      must be unique.\n",
    "    batch_processor: a function taking four arguments:\n",
    "      1. tensor_dict: the same tensor_dict that is passed in as the first\n",
    "        argument to this function.\n",
    "      2. sess: a tensorflow session\n",
    "      3. batch_index: an integer representing the index of the batch amongst\n",
    "        all batches\n",
    "      By default, batch_processor is None, which defaults to running:\n",
    "        return sess.run(tensor_dict)\n",
    "      To skip an image, it suffices to return an empty dictionary in place of\n",
    "      result_dict.\n",
    "    checkpoint_dirs: list of directories to load into an EnsembleModel. If it\n",
    "      has only one directory, EnsembleModel will not be used --\n",
    "        a DetectionModel\n",
    "      will be instantiated directly. Not used if restore_fn is set.\n",
    "    variables_to_restore: None, or a dictionary mapping variable names found in\n",
    "      a checkpoint to model variables. The dictionary would normally be\n",
    "      generated by creating a tf.train.ExponentialMovingAverage object and\n",
    "      calling its variables_to_restore() method. Not used if restore_fn is set.\n",
    "    restore_fn: None, or a function that takes a tf.Session object and correctly\n",
    "      restores all necessary variables from the correct checkpoint file. If\n",
    "      None, attempts to restore from the first directory in checkpoint_dirs.\n",
    "    num_batches: the number of batches to use for evaluation.\n",
    "    master: the location of the Tensorflow session.\n",
    "    save_graph: whether or not the Tensorflow graph is stored as a pbtxt file.\n",
    "    save_graph_dir: where to store the Tensorflow graph on disk. If save_graph\n",
    "      is True this must be non-empty.\n",
    "    losses_dict: optional dictionary of scalar detection losses.\n",
    "    eval_export_path: Path for saving a json file that contains the detection\n",
    "      results in json format.\n",
    "    process_metrics_fn: a callback called with evaluation results after each\n",
    "      evaluation is done.  It could be used e.g. to back up checkpoints with\n",
    "      best evaluation scores, or to call an external system to update evaluation\n",
    "      results in order to drive best hyper-parameter search.  Parameters are:\n",
    "      int checkpoint_number, Dict[str, ObjectDetectionEvalMetrics] metrics,\n",
    "      str checkpoint_file path.\n",
    "\n",
    "  Returns:\n",
    "    global_step: the count of global steps.\n",
    "    all_evaluator_metrics: A dictionary containing metric names and values.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if restore_fn is None and checkpoint_dirs doesn't have at least\n",
    "      one element.\n",
    "    ValueError: if save_graph is True and save_graph_dir is not defined.\n",
    "  \"\"\"\n",
    "  if save_graph and not save_graph_dir:\n",
    "    raise ValueError('`save_graph_dir` must be defined.')\n",
    "  sess = tf.Session(master, graph=tf.get_default_graph())\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  sess.run(tf.local_variables_initializer())\n",
    "  sess.run(tf.tables_initializer())\n",
    "  checkpoint_file = None\n",
    "  if restore_fn:\n",
    "    restore_fn(sess)\n",
    "  else:\n",
    "    if not checkpoint_dirs:\n",
    "      raise ValueError('`checkpoint_dirs` must have at least one entry.')\n",
    "    checkpoint_file = tf.train.latest_checkpoint(checkpoint_dirs[0])\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    saver.restore(sess, checkpoint_file)\n",
    "\n",
    "  if save_graph:\n",
    "    tf.train.write_graph(sess.graph_def, save_graph_dir, 'eval.pbtxt')\n",
    "\n",
    "  counters = {'skipped': 0, 'success': 0}\n",
    "  aggregate_result_losses_dict = collections.defaultdict(list)\n",
    "  with slim.queues.QueueRunners(sess):\n",
    "    try:\n",
    "      for batch in range(int(num_batches)):\n",
    "        if (batch + 1) % 100 == 0:\n",
    "          tf.logging.info('Running eval ops batch %d/%d', batch + 1,\n",
    "                          num_batches)\n",
    "        if not batch_processor:\n",
    "          try:\n",
    "            if not losses_dict:\n",
    "              losses_dict = {}\n",
    "            result_dict, result_losses_dict = sess.run([tensor_dict,\n",
    "                                                        losses_dict])\n",
    "            counters['success'] += 1\n",
    "          except tf.errors.InvalidArgumentError:\n",
    "            tf.logging.info('Skipping image')\n",
    "            counters['skipped'] += 1\n",
    "            result_dict = {}\n",
    "        else:\n",
    "          result_dict, result_losses_dict = batch_processor(\n",
    "              tensor_dict, sess, batch, counters, losses_dict=losses_dict)\n",
    "        if not result_dict:\n",
    "          continue\n",
    "        for key, value in iter(result_losses_dict.items()):\n",
    "          aggregate_result_losses_dict[key].append(value)\n",
    "        for evaluator in evaluators:\n",
    "          # TODO(b/65130867): Use image_id tensor once we fix the input data\n",
    "          # decoders to return correct image_id.\n",
    "          # TODO(akuznetsa): result_dict contains batches of images, while\n",
    "          # add_single_ground_truth_image_info expects a single image. Fix\n",
    "          if (isinstance(result_dict, dict) and\n",
    "              fields.InputDataFields.key in result_dict and\n",
    "              result_dict[fields.InputDataFields.key]):\n",
    "            image_id = result_dict[fields.InputDataFields.key]\n",
    "          else:\n",
    "            image_id = batch\n",
    "          evaluator.add_single_ground_truth_image_info(\n",
    "              image_id=image_id, groundtruth_dict=result_dict)\n",
    "          evaluator.add_single_detected_image_info(\n",
    "              image_id=image_id, detections_dict=result_dict)\n",
    "      tf.logging.info('Running eval batches done.')\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      tf.logging.info('Done evaluating -- epoch limit reached')\n",
    "    finally:\n",
    "      # When done, ask the threads to stop.\n",
    "      tf.logging.info('# success: %d', counters['success'])\n",
    "      tf.logging.info('# skipped: %d', counters['skipped'])\n",
    "      all_evaluator_metrics = {}\n",
    "      if eval_export_path and eval_export_path is not None:\n",
    "        for evaluator in evaluators:\n",
    "          if (isinstance(evaluator, coco_evaluation.CocoDetectionEvaluator) or\n",
    "              isinstance(evaluator, coco_evaluation.CocoMaskEvaluator)):\n",
    "            tf.logging.info('Started dumping to json file.')\n",
    "            evaluator.dump_detections_to_json_file(\n",
    "                json_output_path=eval_export_path)\n",
    "            tf.logging.info('Finished dumping to json file.')\n",
    "      for evaluator in evaluators:\n",
    "        metrics = evaluator.evaluate()\n",
    "        evaluator.clear()\n",
    "        if any(key in all_evaluator_metrics for key in metrics):\n",
    "          raise ValueError('Metric names between evaluators must not collide.')\n",
    "        all_evaluator_metrics.update(metrics)\n",
    "      global_step = tf.train.global_step(sess, tf.train.get_global_step())\n",
    "\n",
    "      for key, value in iter(aggregate_result_losses_dict.items()):\n",
    "        all_evaluator_metrics['Losses/' + key] = np.mean(value)\n",
    "      if process_metrics_fn and checkpoint_file:\n",
    "        m = re.search(r'model.ckpt-(\\d+)$', checkpoint_file)\n",
    "        if not m:\n",
    "          tf.logging.error('Failed to parse checkpoint number from: %s',\n",
    "                           checkpoint_file)\n",
    "        else:\n",
    "          checkpoint_number = int(m.group(1))\n",
    "          process_metrics_fn(checkpoint_number, all_evaluator_metrics,\n",
    "                             checkpoint_file)\n",
    "  sess.close()\n",
    "  return (global_step, all_evaluator_metrics)\n",
    "\n",
    "\n",
    "# TODO(rathodv): Add tests.\n",
    "def repeated_checkpoint_run(tensor_dict,\n",
    "                            summary_dir,\n",
    "                            evaluators,\n",
    "                            batch_processor=None,\n",
    "                            checkpoint_dirs=None,\n",
    "                            variables_to_restore=None,\n",
    "                            restore_fn=None,\n",
    "                            num_batches=1,\n",
    "                            eval_interval_secs=120,\n",
    "                            max_number_of_evaluations=None,\n",
    "                            max_evaluation_global_step=None,\n",
    "                            master='',\n",
    "                            save_graph=False,\n",
    "                            save_graph_dir='',\n",
    "                            losses_dict=None,\n",
    "                            eval_export_path=None,\n",
    "                            process_metrics_fn=None):\n",
    "  \"\"\"Periodically evaluates desired tensors using checkpoint_dirs or restore_fn.\n",
    "\n",
    "  This function repeatedly loads a checkpoint and evaluates a desired\n",
    "  set of tensors (provided by tensor_dict) and hands the resulting numpy\n",
    "  arrays to a function result_processor which can be used to further\n",
    "  process/save/visualize the results.\n",
    "\n",
    "  Args:\n",
    "    tensor_dict: a dictionary holding tensors representing a batch of detections\n",
    "      and corresponding groundtruth annotations.\n",
    "    summary_dir: a directory to write metrics summaries.\n",
    "    evaluators: a list of object of type DetectionEvaluator to be used for\n",
    "      evaluation. Note that the metric names produced by different evaluators\n",
    "      must be unique.\n",
    "    batch_processor: a function taking three arguments:\n",
    "      1. tensor_dict: the same tensor_dict that is passed in as the first\n",
    "        argument to this function.\n",
    "      2. sess: a tensorflow session\n",
    "      3. batch_index: an integer representing the index of the batch amongst\n",
    "        all batches\n",
    "      By default, batch_processor is None, which defaults to running:\n",
    "        return sess.run(tensor_dict)\n",
    "    checkpoint_dirs: list of directories to load into a DetectionModel or an\n",
    "      EnsembleModel if restore_fn isn't set. Also used to determine when to run\n",
    "      next evaluation. Must have at least one element.\n",
    "    variables_to_restore: None, or a dictionary mapping variable names found in\n",
    "      a checkpoint to model variables. The dictionary would normally be\n",
    "      generated by creating a tf.train.ExponentialMovingAverage object and\n",
    "      calling its variables_to_restore() method. Not used if restore_fn is set.\n",
    "    restore_fn: a function that takes a tf.Session object and correctly restores\n",
    "      all necessary variables from the correct checkpoint file.\n",
    "    num_batches: the number of batches to use for evaluation.\n",
    "    eval_interval_secs: the number of seconds between each evaluation run.\n",
    "    max_number_of_evaluations: the max number of iterations of the evaluation.\n",
    "      If the value is left as None the evaluation continues indefinitely.\n",
    "    max_evaluation_global_step: global step when evaluation stops.\n",
    "    master: the location of the Tensorflow session.\n",
    "    save_graph: whether or not the Tensorflow graph is saved as a pbtxt file.\n",
    "    save_graph_dir: where to save on disk the Tensorflow graph. If store_graph\n",
    "      is True this must be non-empty.\n",
    "    losses_dict: optional dictionary of scalar detection losses.\n",
    "    eval_export_path: Path for saving a json file that contains the detection\n",
    "      results in json format.\n",
    "    process_metrics_fn: a callback called with evaluation results after each\n",
    "      evaluation is done.  It could be used e.g. to back up checkpoints with\n",
    "      best evaluation scores, or to call an external system to update evaluation\n",
    "      results in order to drive best hyper-parameter search.  Parameters are:\n",
    "      int checkpoint_number, Dict[str, ObjectDetectionEvalMetrics] metrics,\n",
    "      str checkpoint_file path.\n",
    "\n",
    "  Returns:\n",
    "    metrics: A dictionary containing metric names and values in the latest\n",
    "      evaluation.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if max_num_of_evaluations is not None or a positive number.\n",
    "    ValueError: if checkpoint_dirs doesn't have at least one element.\n",
    "  \"\"\"\n",
    "  if max_number_of_evaluations and max_number_of_evaluations <= 0:\n",
    "    raise ValueError(\n",
    "        '`max_number_of_evaluations` must be either None or a positive number.')\n",
    "  if max_evaluation_global_step and max_evaluation_global_step <= 0:\n",
    "    raise ValueError(\n",
    "        '`max_evaluation_global_step` must be either None or positive.')\n",
    "\n",
    "  if not checkpoint_dirs:\n",
    "    raise ValueError('`checkpoint_dirs` must have at least one entry.')\n",
    "\n",
    "  last_evaluated_model_path = None\n",
    "  number_of_evaluations = 0\n",
    "  while True:\n",
    "    start = time.time()\n",
    "    tf.logging.info('Starting evaluation at ' + time.strftime(\n",
    "        '%Y-%m-%d-%H:%M:%S', time.gmtime()))\n",
    "    model_path = tf.train.latest_checkpoint(checkpoint_dirs[0])\n",
    "    if not model_path:\n",
    "      tf.logging.info('No model found in %s. Will try again in %d seconds',\n",
    "                      checkpoint_dirs[0], eval_interval_secs)\n",
    "    elif model_path == last_evaluated_model_path:\n",
    "      tf.logging.info('Found already evaluated checkpoint. Will try again in '\n",
    "                      '%d seconds', eval_interval_secs)\n",
    "    else:\n",
    "      last_evaluated_model_path = model_path\n",
    "      global_step, metrics = _run_checkpoint_once(\n",
    "          tensor_dict,\n",
    "          evaluators,\n",
    "          batch_processor,\n",
    "          checkpoint_dirs,\n",
    "          variables_to_restore,\n",
    "          restore_fn,\n",
    "          num_batches,\n",
    "          master,\n",
    "          save_graph,\n",
    "          save_graph_dir,\n",
    "          losses_dict=losses_dict,\n",
    "          eval_export_path=eval_export_path,\n",
    "          process_metrics_fn=process_metrics_fn)\n",
    "      write_metrics(metrics, global_step, summary_dir)\n",
    "      if (max_evaluation_global_step and\n",
    "          global_step >= max_evaluation_global_step):\n",
    "        tf.logging.info('Finished evaluation!')\n",
    "        break\n",
    "    number_of_evaluations += 1\n",
    "\n",
    "    if (max_number_of_evaluations and\n",
    "        number_of_evaluations >= max_number_of_evaluations):\n",
    "      tf.logging.info('Finished evaluation!')\n",
    "      break\n",
    "    time_to_next_eval = start + eval_interval_secs - time.time()\n",
    "    if time_to_next_eval > 0:\n",
    "      time.sleep(time_to_next_eval)\n",
    "\n",
    "  return metrics\n",
    "\n",
    "\n",
    "def _scale_box_to_absolute(args):\n",
    "  boxes, image_shape = args\n",
    "  return box_list_ops.to_absolute_coordinates(\n",
    "      box_list.BoxList(boxes), image_shape[0], image_shape[1]).get()\n",
    "\n",
    "\n",
    "def _resize_detection_masks(arg_tuple):\n",
    "  \"\"\"Resizes detection masks.\n",
    "\n",
    "  Args:\n",
    "    arg_tuple: A (detection_boxes, detection_masks, image_shape, pad_shape)\n",
    "      tuple where\n",
    "      detection_boxes is a tf.float32 tensor of size [num_masks, 4] containing\n",
    "        the box corners. Row i contains [ymin, xmin, ymax, xmax] of the box\n",
    "        corresponding to mask i. Note that the box corners are in\n",
    "        normalized coordinates.\n",
    "      detection_masks is a tensor of size\n",
    "        [num_masks, mask_height, mask_width].\n",
    "      image_shape is a tensor of shape [2]\n",
    "      pad_shape is a tensor of shape [2] --- this is assumed to be greater\n",
    "        than or equal to image_shape along both dimensions and represents a\n",
    "        shape to-be-padded-to.\n",
    "\n",
    "  Returns:\n",
    "  \"\"\"\n",
    "\n",
    "  detection_boxes, detection_masks, image_shape, pad_shape = arg_tuple\n",
    "\n",
    "  detection_masks_reframed = ops.reframe_box_masks_to_image_masks(\n",
    "      detection_masks, detection_boxes, image_shape[0], image_shape[1])\n",
    "\n",
    "  pad_instance_dim = tf.zeros([3, 1], dtype=tf.int32)\n",
    "  pad_hw_dim = tf.concat([tf.zeros([1], dtype=tf.int32),\n",
    "                          pad_shape - image_shape], axis=0)\n",
    "  pad_hw_dim = tf.expand_dims(pad_hw_dim, 1)\n",
    "  paddings = tf.concat([pad_instance_dim, pad_hw_dim], axis=1)\n",
    "  detection_masks_reframed = tf.pad(detection_masks_reframed, paddings)\n",
    "\n",
    "  # If the masks are currently float, binarize them. Otherwise keep them as\n",
    "  # integers, since they have already been thresholded.\n",
    "  if detection_masks_reframed.dtype == tf.float32:\n",
    "    detection_masks_reframed = tf.greater(detection_masks_reframed, 0.5)\n",
    "  return tf.cast(detection_masks_reframed, tf.uint8)\n",
    "\n",
    "\n",
    "def resize_detection_masks(detection_boxes, detection_masks,\n",
    "                           original_image_spatial_shapes):\n",
    "  \"\"\"Resizes per-box detection masks to be relative to the entire image.\n",
    "\n",
    "  Note that this function only works when the spatial size of all images in\n",
    "  the batch is the same. If not, this function should be used with batch_size=1.\n",
    "\n",
    "  Args:\n",
    "    detection_boxes: A [batch_size, num_instances, 4] float tensor containing\n",
    "      bounding boxes.\n",
    "    detection_masks: A [batch_size, num_instances, height, width] float tensor\n",
    "      containing binary instance masks per box.\n",
    "    original_image_spatial_shapes: a [batch_size, 3] shaped int tensor\n",
    "      holding the spatial dimensions of each image in the batch.\n",
    "  Returns:\n",
    "    masks: Masks resized to the spatial extents given by\n",
    "      (original_image_spatial_shapes[0, 0], original_image_spatial_shapes[0, 1])\n",
    "  \"\"\"\n",
    "  # modify original image spatial shapes to be max along each dim\n",
    "  # in evaluator, should have access to original_image_spatial_shape field\n",
    "  # in add_Eval_Dict\n",
    "  max_spatial_shape = tf.reduce_max(\n",
    "      original_image_spatial_shapes, axis=0, keep_dims=True)\n",
    "  tiled_max_spatial_shape = tf.tile(\n",
    "      max_spatial_shape,\n",
    "      multiples=[tf.shape(original_image_spatial_shapes)[0], 1])\n",
    "  return shape_utils.static_or_dynamic_map_fn(\n",
    "      _resize_detection_masks,\n",
    "      elems=[detection_boxes,\n",
    "             detection_masks,\n",
    "             original_image_spatial_shapes,\n",
    "             tiled_max_spatial_shape],\n",
    "      dtype=tf.uint8)\n",
    "\n",
    "\n",
    "def _resize_groundtruth_masks(args):\n",
    "  \"\"\"Resizes groundtruth masks to the original image size.\"\"\"\n",
    "  mask, true_image_shape, original_image_shape, pad_shape = args\n",
    "  true_height = true_image_shape[0]\n",
    "  true_width = true_image_shape[1]\n",
    "  mask = mask[:, :true_height, :true_width]\n",
    "  mask = tf.expand_dims(mask, 3)\n",
    "  mask = tf.image.resize_images(\n",
    "      mask,\n",
    "      original_image_shape,\n",
    "      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "      align_corners=True)\n",
    "\n",
    "  paddings = tf.concat(\n",
    "      [tf.zeros([3, 1], dtype=tf.int32),\n",
    "       tf.expand_dims(\n",
    "           tf.concat([tf.zeros([1], dtype=tf.int32),\n",
    "                      pad_shape-original_image_shape], axis=0),\n",
    "           1)], axis=1)\n",
    "  mask = tf.pad(tf.squeeze(mask, 3), paddings)\n",
    "  return tf.cast(mask, tf.uint8)\n",
    "\n",
    "\n",
    "def _resize_surface_coordinate_masks(args):\n",
    "  detection_boxes, surface_coords, image_shape = args\n",
    "  surface_coords_v, surface_coords_u = tf.unstack(surface_coords, axis=-1)\n",
    "  surface_coords_v_reframed = ops.reframe_box_masks_to_image_masks(\n",
    "      surface_coords_v, detection_boxes, image_shape[0], image_shape[1])\n",
    "  surface_coords_u_reframed = ops.reframe_box_masks_to_image_masks(\n",
    "      surface_coords_u, detection_boxes, image_shape[0], image_shape[1])\n",
    "  return tf.stack([surface_coords_v_reframed, surface_coords_u_reframed],\n",
    "                  axis=-1)\n",
    "\n",
    "\n",
    "def _scale_keypoint_to_absolute(args):\n",
    "  keypoints, image_shape = args\n",
    "  return keypoint_ops.scale(keypoints, image_shape[0], image_shape[1])\n",
    "\n",
    "\n",
    "def result_dict_for_single_example(image,\n",
    "                                   key,\n",
    "                                   detections,\n",
    "                                   groundtruth=None,\n",
    "                                   class_agnostic=False,\n",
    "                                   scale_to_absolute=False):\n",
    "  \"\"\"Merges all detection and groundtruth information for a single example.\n",
    "\n",
    "  Note that evaluation tools require classes that are 1-indexed, and so this\n",
    "  function performs the offset. If `class_agnostic` is True, all output classes\n",
    "  have label 1.\n",
    "\n",
    "  Args:\n",
    "    image: A single 4D uint8 image tensor of shape [1, H, W, C].\n",
    "    key: A single string tensor identifying the image.\n",
    "    detections: A dictionary of detections, returned from\n",
    "      DetectionModel.postprocess().\n",
    "    groundtruth: (Optional) Dictionary of groundtruth items, with fields:\n",
    "      'groundtruth_boxes': [num_boxes, 4] float32 tensor of boxes, in\n",
    "        normalized coordinates.\n",
    "      'groundtruth_classes': [num_boxes] int64 tensor of 1-indexed classes.\n",
    "      'groundtruth_area': [num_boxes] float32 tensor of bbox area. (Optional)\n",
    "      'groundtruth_is_crowd': [num_boxes] int64 tensor. (Optional)\n",
    "      'groundtruth_difficult': [num_boxes] int64 tensor. (Optional)\n",
    "      'groundtruth_group_of': [num_boxes] int64 tensor. (Optional)\n",
    "      'groundtruth_instance_masks': 3D int64 tensor of instance masks\n",
    "        (Optional).\n",
    "      'groundtruth_keypoints': [num_boxes, num_keypoints, 2] float32 tensor with\n",
    "        keypoints (Optional).\n",
    "    class_agnostic: Boolean indicating whether the detections are class-agnostic\n",
    "      (i.e. binary). Default False.\n",
    "    scale_to_absolute: Boolean indicating whether boxes and keypoints should be\n",
    "      scaled to absolute coordinates. Note that for IoU based evaluations, it\n",
    "      does not matter whether boxes are expressed in absolute or relative\n",
    "      coordinates. Default False.\n",
    "\n",
    "  Returns:\n",
    "    A dictionary with:\n",
    "    'original_image': A [1, H, W, C] uint8 image tensor.\n",
    "    'key': A string tensor with image identifier.\n",
    "    'detection_boxes': [max_detections, 4] float32 tensor of boxes, in\n",
    "      normalized or absolute coordinates, depending on the value of\n",
    "      `scale_to_absolute`.\n",
    "    'detection_scores': [max_detections] float32 tensor of scores.\n",
    "    'detection_classes': [max_detections] int64 tensor of 1-indexed classes.\n",
    "    'detection_masks': [max_detections, H, W] float32 tensor of binarized\n",
    "      masks, reframed to full image masks.\n",
    "    'groundtruth_boxes': [num_boxes, 4] float32 tensor of boxes, in\n",
    "      normalized or absolute coordinates, depending on the value of\n",
    "      `scale_to_absolute`. (Optional)\n",
    "    'groundtruth_classes': [num_boxes] int64 tensor of 1-indexed classes.\n",
    "      (Optional)\n",
    "    'groundtruth_area': [num_boxes] float32 tensor of bbox area. (Optional)\n",
    "    'groundtruth_is_crowd': [num_boxes] int64 tensor. (Optional)\n",
    "    'groundtruth_difficult': [num_boxes] int64 tensor. (Optional)\n",
    "    'groundtruth_group_of': [num_boxes] int64 tensor. (Optional)\n",
    "    'groundtruth_instance_masks': 3D int64 tensor of instance masks\n",
    "      (Optional).\n",
    "    'groundtruth_keypoints': [num_boxes, num_keypoints, 2] float32 tensor with\n",
    "      keypoints (Optional).\n",
    "  \"\"\"\n",
    "\n",
    "  if groundtruth:\n",
    "    max_gt_boxes = tf.shape(\n",
    "        groundtruth[fields.InputDataFields.groundtruth_boxes])[0]\n",
    "    for gt_key in groundtruth:\n",
    "      # expand groundtruth dict along the batch dimension.\n",
    "      groundtruth[gt_key] = tf.expand_dims(groundtruth[gt_key], 0)\n",
    "\n",
    "  for detection_key in detections:\n",
    "    detections[detection_key] = tf.expand_dims(\n",
    "        detections[detection_key][0], axis=0)\n",
    "\n",
    "  batched_output_dict = result_dict_for_batched_example(\n",
    "      image,\n",
    "      tf.expand_dims(key, 0),\n",
    "      detections,\n",
    "      groundtruth,\n",
    "      class_agnostic,\n",
    "      scale_to_absolute,\n",
    "      max_gt_boxes=max_gt_boxes)\n",
    "\n",
    "  exclude_keys = [\n",
    "      fields.InputDataFields.original_image,\n",
    "      fields.DetectionResultFields.num_detections,\n",
    "      fields.InputDataFields.num_groundtruth_boxes\n",
    "  ]\n",
    "\n",
    "  output_dict = {\n",
    "      fields.InputDataFields.original_image:\n",
    "          batched_output_dict[fields.InputDataFields.original_image]\n",
    "  }\n",
    "\n",
    "  for key in batched_output_dict:\n",
    "    # remove the batch dimension.\n",
    "    if key not in exclude_keys:\n",
    "      output_dict[key] = tf.squeeze(batched_output_dict[key], 0)\n",
    "  return output_dict\n",
    "\n",
    "\n",
    "def result_dict_for_batched_example(images,\n",
    "                                    keys,\n",
    "                                    detections,\n",
    "                                    groundtruth=None,\n",
    "                                    class_agnostic=False,\n",
    "                                    scale_to_absolute=False,\n",
    "                                    original_image_spatial_shapes=None,\n",
    "                                    true_image_shapes=None,\n",
    "                                    max_gt_boxes=None,\n",
    "                                    label_id_offset=1):\n",
    "  \"\"\"Merges all detection and groundtruth information for a single example.\n",
    "\n",
    "  Note that evaluation tools require classes that are 1-indexed, and so this\n",
    "  function performs the offset. If `class_agnostic` is True, all output classes\n",
    "  have label 1.\n",
    "  The groundtruth coordinates of boxes/keypoints in 'groundtruth' dictionary are\n",
    "  normalized relative to the (potentially padded) input image, while the\n",
    "  coordinates in 'detection' dictionary are normalized relative to the true\n",
    "  image shape.\n",
    "\n",
    "  Args:\n",
    "    images: A single 4D uint8 image tensor of shape [batch_size, H, W, C].\n",
    "    keys: A [batch_size] string/int tensor with image identifier.\n",
    "    detections: A dictionary of detections, returned from\n",
    "      DetectionModel.postprocess().\n",
    "    groundtruth: (Optional) Dictionary of groundtruth items, with fields:\n",
    "      'groundtruth_boxes': [batch_size, max_number_of_boxes, 4] float32 tensor\n",
    "        of boxes, in normalized coordinates.\n",
    "      'groundtruth_classes':  [batch_size, max_number_of_boxes] int64 tensor of\n",
    "        1-indexed classes.\n",
    "      'groundtruth_area': [batch_size, max_number_of_boxes] float32 tensor of\n",
    "        bbox area. (Optional)\n",
    "      'groundtruth_is_crowd':[batch_size, max_number_of_boxes] int64\n",
    "        tensor. (Optional)\n",
    "      'groundtruth_difficult': [batch_size, max_number_of_boxes] int64\n",
    "        tensor. (Optional)\n",
    "      'groundtruth_group_of': [batch_size, max_number_of_boxes] int64\n",
    "        tensor. (Optional)\n",
    "      'groundtruth_instance_masks': 4D int64 tensor of instance\n",
    "        masks (Optional).\n",
    "      'groundtruth_keypoints': [batch_size, max_number_of_boxes, num_keypoints,\n",
    "        2] float32 tensor with keypoints (Optional).\n",
    "      'groundtruth_keypoint_visibilities': [batch_size, max_number_of_boxes,\n",
    "        num_keypoints] bool tensor with keypoint visibilities (Optional).\n",
    "      'groundtruth_labeled_classes': [batch_size, num_classes] int64\n",
    "        tensor of 1-indexed classes. (Optional)\n",
    "      'groundtruth_dp_num_points': [batch_size, max_number_of_boxes] int32\n",
    "        tensor. (Optional)\n",
    "      'groundtruth_dp_part_ids': [batch_size, max_number_of_boxes,\n",
    "        max_sampled_points] int32 tensor. (Optional)\n",
    "      'groundtruth_dp_surface_coords_list': [batch_size, max_number_of_boxes,\n",
    "        max_sampled_points, 4] float32 tensor. (Optional)\n",
    "    class_agnostic: Boolean indicating whether the detections are class-agnostic\n",
    "      (i.e. binary). Default False.\n",
    "    scale_to_absolute: Boolean indicating whether boxes and keypoints should be\n",
    "      scaled to absolute coordinates. Note that for IoU based evaluations, it\n",
    "      does not matter whether boxes are expressed in absolute or relative\n",
    "      coordinates. Default False.\n",
    "    original_image_spatial_shapes: A 2D int32 tensor of shape [batch_size, 2]\n",
    "      used to resize the image. When set to None, the image size is retained.\n",
    "    true_image_shapes: A 2D int32 tensor of shape [batch_size, 3]\n",
    "      containing the size of the unpadded original_image.\n",
    "    max_gt_boxes: [batch_size] tensor representing the maximum number of\n",
    "      groundtruth boxes to pad.\n",
    "    label_id_offset: offset for class ids.\n",
    "\n",
    "  Returns:\n",
    "    A dictionary with:\n",
    "    'original_image': A [batch_size, H, W, C] uint8 image tensor.\n",
    "    'original_image_spatial_shape': A [batch_size, 2] tensor containing the\n",
    "      original image sizes.\n",
    "    'true_image_shape': A [batch_size, 3] tensor containing the size of\n",
    "      the unpadded original_image.\n",
    "    'key': A [batch_size] string tensor with image identifier.\n",
    "    'detection_boxes': [batch_size, max_detections, 4] float32 tensor of boxes,\n",
    "      in normalized or absolute coordinates, depending on the value of\n",
    "      `scale_to_absolute`.\n",
    "    'detection_scores': [batch_size, max_detections] float32 tensor of scores.\n",
    "    'detection_classes': [batch_size, max_detections] int64 tensor of 1-indexed\n",
    "      classes.\n",
    "    'detection_masks': [batch_size, max_detections, H, W] uint8 tensor of\n",
    "      instance masks, reframed to full image masks. Note that these may be\n",
    "      binarized (e.g. {0, 1}), or may contain 1-indexed part labels. (Optional)\n",
    "    'detection_keypoints': [batch_size, max_detections, num_keypoints, 2]\n",
    "      float32 tensor containing keypoint coordinates. (Optional)\n",
    "    'detection_keypoint_scores': [batch_size, max_detections, num_keypoints]\n",
    "      float32 tensor containing keypoint scores. (Optional)\n",
    "    'detection_surface_coords': [batch_size, max_detection, H, W, 2] float32\n",
    "      tensor with normalized surface coordinates (e.g. DensePose UV\n",
    "      coordinates). (Optional)\n",
    "    'num_detections': [batch_size] int64 tensor containing number of valid\n",
    "      detections.\n",
    "    'groundtruth_boxes': [batch_size, num_boxes, 4] float32 tensor of boxes, in\n",
    "      normalized or absolute coordinates, depending on the value of\n",
    "      `scale_to_absolute`. (Optional)\n",
    "    'groundtruth_classes': [batch_size, num_boxes] int64 tensor of 1-indexed\n",
    "      classes. (Optional)\n",
    "    'groundtruth_area': [batch_size, num_boxes] float32 tensor of bbox\n",
    "      area. (Optional)\n",
    "    'groundtruth_is_crowd': [batch_size, num_boxes] int64 tensor. (Optional)\n",
    "    'groundtruth_difficult': [batch_size, num_boxes] int64 tensor. (Optional)\n",
    "    'groundtruth_group_of': [batch_size, num_boxes] int64 tensor. (Optional)\n",
    "    'groundtruth_instance_masks': 4D int64 tensor of instance masks\n",
    "      (Optional).\n",
    "    'groundtruth_keypoints': [batch_size, num_boxes, num_keypoints, 2] float32\n",
    "      tensor with keypoints (Optional).\n",
    "    'groundtruth_keypoint_visibilities': [batch_size, num_boxes, num_keypoints]\n",
    "      bool tensor with keypoint visibilities (Optional).\n",
    "    'groundtruth_labeled_classes': [batch_size, num_classes]  int64 tensor\n",
    "      of 1-indexed classes. (Optional)\n",
    "    'num_groundtruth_boxes': [batch_size] tensor containing the maximum number\n",
    "      of groundtruth boxes per image.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if original_image_spatial_shape is not 2D int32 tensor of shape\n",
    "      [2].\n",
    "    ValueError: if true_image_shapes is not 2D int32 tensor of shape\n",
    "      [3].\n",
    "  \"\"\"\n",
    "  input_data_fields = fields.InputDataFields\n",
    "  if original_image_spatial_shapes is None:\n",
    "    original_image_spatial_shapes = tf.tile(\n",
    "        tf.expand_dims(tf.shape(images)[1:3], axis=0),\n",
    "        multiples=[tf.shape(images)[0], 1])\n",
    "  else:\n",
    "    if (len(original_image_spatial_shapes.shape) != 2 and\n",
    "        original_image_spatial_shapes.shape[1] != 2):\n",
    "      raise ValueError(\n",
    "          '`original_image_spatial_shape` should be a 2D tensor of shape '\n",
    "          '[batch_size, 2].')\n",
    "\n",
    "  if true_image_shapes is None:\n",
    "    true_image_shapes = tf.tile(\n",
    "        tf.expand_dims(tf.shape(images)[1:4], axis=0),\n",
    "        multiples=[tf.shape(images)[0], 1])\n",
    "  else:\n",
    "    if (len(true_image_shapes.shape) != 2\n",
    "        and true_image_shapes.shape[1] != 3):\n",
    "      raise ValueError('`true_image_shapes` should be a 2D tensor of '\n",
    "                       'shape [batch_size, 3].')\n",
    "\n",
    "  output_dict = {\n",
    "      input_data_fields.original_image:\n",
    "          images,\n",
    "      input_data_fields.key:\n",
    "          keys,\n",
    "      input_data_fields.original_image_spatial_shape: (\n",
    "          original_image_spatial_shapes),\n",
    "      input_data_fields.true_image_shape:\n",
    "          true_image_shapes\n",
    "  }\n",
    "\n",
    "  detection_fields = fields.DetectionResultFields\n",
    "  detection_boxes = detections[detection_fields.detection_boxes]\n",
    "  detection_scores = detections[detection_fields.detection_scores]\n",
    "  num_detections = tf.cast(detections[detection_fields.num_detections],\n",
    "                           dtype=tf.int32)\n",
    "\n",
    "  if class_agnostic:\n",
    "    detection_classes = tf.ones_like(detection_scores, dtype=tf.int64)\n",
    "  else:\n",
    "    detection_classes = (\n",
    "        tf.to_int64(detections[detection_fields.detection_classes]) +\n",
    "        label_id_offset)\n",
    "\n",
    "  if scale_to_absolute:\n",
    "    output_dict[detection_fields.detection_boxes] = (\n",
    "        shape_utils.static_or_dynamic_map_fn(\n",
    "            _scale_box_to_absolute,\n",
    "            elems=[detection_boxes, original_image_spatial_shapes],\n",
    "            dtype=tf.float32))\n",
    "  else:\n",
    "    output_dict[detection_fields.detection_boxes] = detection_boxes\n",
    "  output_dict[detection_fields.detection_classes] = detection_classes\n",
    "  output_dict[detection_fields.detection_scores] = detection_scores\n",
    "  output_dict[detection_fields.num_detections] = num_detections\n",
    "\n",
    "  if detection_fields.detection_masks in detections:\n",
    "    detection_masks = detections[detection_fields.detection_masks]\n",
    "    output_dict[detection_fields.detection_masks] = resize_detection_masks(\n",
    "        detection_boxes, detection_masks, original_image_spatial_shapes)\n",
    "\n",
    "    if detection_fields.detection_surface_coords in detections:\n",
    "      detection_surface_coords = detections[\n",
    "          detection_fields.detection_surface_coords]\n",
    "      output_dict[detection_fields.detection_surface_coords] = (\n",
    "          shape_utils.static_or_dynamic_map_fn(\n",
    "              _resize_surface_coordinate_masks,\n",
    "              elems=[detection_boxes, detection_surface_coords,\n",
    "                     original_image_spatial_shapes],\n",
    "              dtype=tf.float32))\n",
    "\n",
    "  if detection_fields.detection_keypoints in detections:\n",
    "    detection_keypoints = detections[detection_fields.detection_keypoints]\n",
    "    output_dict[detection_fields.detection_keypoints] = detection_keypoints\n",
    "    if scale_to_absolute:\n",
    "      output_dict[detection_fields.detection_keypoints] = (\n",
    "          shape_utils.static_or_dynamic_map_fn(\n",
    "              _scale_keypoint_to_absolute,\n",
    "              elems=[detection_keypoints, original_image_spatial_shapes],\n",
    "              dtype=tf.float32))\n",
    "    if detection_fields.detection_keypoint_scores in detections:\n",
    "      output_dict[detection_fields.detection_keypoint_scores] = detections[\n",
    "          detection_fields.detection_keypoint_scores]\n",
    "    else:\n",
    "      output_dict[detection_fields.detection_keypoint_scores] = tf.ones_like(\n",
    "          detections[detection_fields.detection_keypoints][:, :, :, 0])\n",
    "\n",
    "  if groundtruth:\n",
    "    if max_gt_boxes is None:\n",
    "      if input_data_fields.num_groundtruth_boxes in groundtruth:\n",
    "        max_gt_boxes = groundtruth[input_data_fields.num_groundtruth_boxes]\n",
    "      else:\n",
    "        raise ValueError(\n",
    "            'max_gt_boxes must be provided when processing batched examples.')\n",
    "\n",
    "    if input_data_fields.groundtruth_instance_masks in groundtruth:\n",
    "      masks = groundtruth[input_data_fields.groundtruth_instance_masks]\n",
    "      max_spatial_shape = tf.reduce_max(\n",
    "          original_image_spatial_shapes, axis=0, keep_dims=True)\n",
    "      tiled_max_spatial_shape = tf.tile(\n",
    "          max_spatial_shape,\n",
    "          multiples=[tf.shape(original_image_spatial_shapes)[0], 1])\n",
    "      groundtruth[input_data_fields.groundtruth_instance_masks] = (\n",
    "          shape_utils.static_or_dynamic_map_fn(\n",
    "              _resize_groundtruth_masks,\n",
    "              elems=[masks, true_image_shapes,\n",
    "                     original_image_spatial_shapes,\n",
    "                     tiled_max_spatial_shape],\n",
    "              dtype=tf.uint8))\n",
    "\n",
    "    output_dict.update(groundtruth)\n",
    "\n",
    "    image_shape = tf.cast(tf.shape(images), tf.float32)\n",
    "    image_height, image_width = image_shape[1], image_shape[2]\n",
    "\n",
    "    def _scale_box_to_normalized_true_image(args):\n",
    "      \"\"\"Scale the box coordinates to be relative to the true image shape.\"\"\"\n",
    "      boxes, true_image_shape = args\n",
    "      true_image_shape = tf.cast(true_image_shape, tf.float32)\n",
    "      true_height, true_width = true_image_shape[0], true_image_shape[1]\n",
    "      normalized_window = tf.stack([0.0, 0.0, true_height / image_height,\n",
    "                                    true_width / image_width])\n",
    "      return box_list_ops.change_coordinate_frame(\n",
    "          box_list.BoxList(boxes), normalized_window).get()\n",
    "\n",
    "    groundtruth_boxes = groundtruth[input_data_fields.groundtruth_boxes]\n",
    "    groundtruth_boxes = shape_utils.static_or_dynamic_map_fn(\n",
    "        _scale_box_to_normalized_true_image,\n",
    "        elems=[groundtruth_boxes, true_image_shapes], dtype=tf.float32)\n",
    "    output_dict[input_data_fields.groundtruth_boxes] = groundtruth_boxes\n",
    "\n",
    "    if input_data_fields.groundtruth_keypoints in groundtruth:\n",
    "      # If groundtruth_keypoints is in the groundtruth dictionary. Update the\n",
    "      # coordinates to conform with the true image shape.\n",
    "      def _scale_keypoints_to_normalized_true_image(args):\n",
    "        \"\"\"Scale the box coordinates to be relative to the true image shape.\"\"\"\n",
    "        keypoints, true_image_shape = args\n",
    "        true_image_shape = tf.cast(true_image_shape, tf.float32)\n",
    "        true_height, true_width = true_image_shape[0], true_image_shape[1]\n",
    "        normalized_window = tf.stack(\n",
    "            [0.0, 0.0, true_height / image_height, true_width / image_width])\n",
    "        return keypoint_ops.change_coordinate_frame(keypoints,\n",
    "                                                    normalized_window)\n",
    "\n",
    "      groundtruth_keypoints = groundtruth[\n",
    "          input_data_fields.groundtruth_keypoints]\n",
    "      groundtruth_keypoints = shape_utils.static_or_dynamic_map_fn(\n",
    "          _scale_keypoints_to_normalized_true_image,\n",
    "          elems=[groundtruth_keypoints, true_image_shapes],\n",
    "          dtype=tf.float32)\n",
    "      output_dict[\n",
    "          input_data_fields.groundtruth_keypoints] = groundtruth_keypoints\n",
    "\n",
    "    if scale_to_absolute:\n",
    "      groundtruth_boxes = output_dict[input_data_fields.groundtruth_boxes]\n",
    "      output_dict[input_data_fields.groundtruth_boxes] = (\n",
    "          shape_utils.static_or_dynamic_map_fn(\n",
    "              _scale_box_to_absolute,\n",
    "              elems=[groundtruth_boxes, original_image_spatial_shapes],\n",
    "              dtype=tf.float32))\n",
    "      if input_data_fields.groundtruth_keypoints in groundtruth:\n",
    "        groundtruth_keypoints = output_dict[\n",
    "            input_data_fields.groundtruth_keypoints]\n",
    "        output_dict[input_data_fields.groundtruth_keypoints] = (\n",
    "            shape_utils.static_or_dynamic_map_fn(\n",
    "                _scale_keypoint_to_absolute,\n",
    "                elems=[groundtruth_keypoints, original_image_spatial_shapes],\n",
    "                dtype=tf.float32))\n",
    "\n",
    "    # For class-agnostic models, groundtruth classes all become 1.\n",
    "    if class_agnostic:\n",
    "      groundtruth_classes = groundtruth[input_data_fields.groundtruth_classes]\n",
    "      groundtruth_classes = tf.ones_like(groundtruth_classes, dtype=tf.int64)\n",
    "      output_dict[input_data_fields.groundtruth_classes] = groundtruth_classes\n",
    "\n",
    "    output_dict[input_data_fields.num_groundtruth_boxes] = max_gt_boxes\n",
    "\n",
    "  return output_dict\n",
    "\n",
    "\n",
    "def get_evaluators(eval_config, categories, evaluator_options=None):\n",
    "  \"\"\"Returns the evaluator class according to eval_config, valid for categories.\n",
    "\n",
    "  Args:\n",
    "    eval_config: An `eval_pb2.EvalConfig`.\n",
    "    categories: A list of dicts, each of which has the following keys -\n",
    "        'id': (required) an integer id uniquely identifying this category.\n",
    "        'name': (required) string representing category name e.g., 'cat', 'dog'.\n",
    "        'keypoints': (optional) dict mapping this category's keypoints to unique\n",
    "          ids.\n",
    "    evaluator_options: A dictionary of metric names (see\n",
    "      EVAL_METRICS_CLASS_DICT) to `DetectionEvaluator` initialization\n",
    "      keyword arguments. For example:\n",
    "      evalator_options = {\n",
    "        'coco_detection_metrics': {'include_metrics_per_category': True}\n",
    "      }\n",
    "\n",
    "  Returns:\n",
    "    An list of instances of DetectionEvaluator.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if metric is not in the metric class dictionary.\n",
    "  \"\"\"\n",
    "  evaluator_options = evaluator_options or {}\n",
    "  eval_metric_fn_keys = eval_config.metrics_set\n",
    "  if not eval_metric_fn_keys:\n",
    "    eval_metric_fn_keys = [EVAL_DEFAULT_METRIC]\n",
    "  evaluators_list = []\n",
    "  for eval_metric_fn_key in eval_metric_fn_keys:\n",
    "    if eval_metric_fn_key not in EVAL_METRICS_CLASS_DICT:\n",
    "      raise ValueError('Metric not found: {}'.format(eval_metric_fn_key))\n",
    "    kwargs_dict = (evaluator_options[eval_metric_fn_key] if eval_metric_fn_key\n",
    "                   in evaluator_options else {})\n",
    "    evaluators_list.append(EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](\n",
    "        categories,\n",
    "        **kwargs_dict))\n",
    "\n",
    "  if isinstance(eval_config, eval_pb2.EvalConfig):\n",
    "    parameterized_metrics = eval_config.parameterized_metric\n",
    "    for parameterized_metric in parameterized_metrics:\n",
    "      assert parameterized_metric.HasField('parameterized_metric')\n",
    "      if parameterized_metric.WhichOneof(\n",
    "          'parameterized_metric') == EVAL_KEYPOINT_METRIC:\n",
    "        keypoint_metrics = parameterized_metric.coco_keypoint_metrics\n",
    "        # Create category to keypoints mapping dict.\n",
    "        category_keypoints = {}\n",
    "        class_label = keypoint_metrics.class_label\n",
    "        category = None\n",
    "        for cat in categories:\n",
    "          if cat['name'] == class_label:\n",
    "            category = cat\n",
    "            break\n",
    "        if not category:\n",
    "          continue\n",
    "        keypoints_for_this_class = category['keypoints']\n",
    "        category_keypoints = [{\n",
    "            'id': keypoints_for_this_class[kp_name], 'name': kp_name\n",
    "        } for kp_name in keypoints_for_this_class]\n",
    "        # Create keypoint evaluator for this category.\n",
    "        evaluators_list.append(EVAL_METRICS_CLASS_DICT[EVAL_KEYPOINT_METRIC](\n",
    "            category['id'], category_keypoints, class_label,\n",
    "            keypoint_metrics.keypoint_label_to_sigmas))\n",
    "  return evaluators_list\n",
    "\n",
    "\n",
    "def get_eval_metric_ops_for_evaluators(eval_config,\n",
    "                                       categories,\n",
    "                                       eval_dict):\n",
    "  \"\"\"Returns eval metrics ops to use with `tf.estimator.EstimatorSpec`.\n",
    "\n",
    "  Args:\n",
    "    eval_config: An `eval_pb2.EvalConfig`.\n",
    "    categories: A list of dicts, each of which has the following keys -\n",
    "        'id': (required) an integer id uniquely identifying this category.\n",
    "        'name': (required) string representing category name e.g., 'cat', 'dog'.\n",
    "    eval_dict: An evaluation dictionary, returned from\n",
    "      result_dict_for_single_example().\n",
    "\n",
    "  Returns:\n",
    "    A dictionary of metric names to tuple of value_op and update_op that can be\n",
    "    used as eval metric ops in tf.EstimatorSpec.\n",
    "  \"\"\"\n",
    "  eval_metric_ops = {}\n",
    "  evaluator_options = evaluator_options_from_eval_config(eval_config)\n",
    "  evaluators_list = get_evaluators(eval_config, categories, evaluator_options)\n",
    "  for evaluator in evaluators_list:\n",
    "    eval_metric_ops.update(evaluator.get_estimator_eval_metric_ops(\n",
    "        eval_dict))\n",
    "  return eval_metric_ops\n",
    "\n",
    "\n",
    "def evaluator_options_from_eval_config(eval_config):\n",
    "  \"\"\"Produces a dictionary of evaluation options for each eval metric.\n",
    "\n",
    "  Args:\n",
    "    eval_config: An `eval_pb2.EvalConfig`.\n",
    "\n",
    "  Returns:\n",
    "    evaluator_options: A dictionary of metric names (see\n",
    "      EVAL_METRICS_CLASS_DICT) to `DetectionEvaluator` initialization\n",
    "      keyword arguments. For example:\n",
    "      evalator_options = {\n",
    "        'coco_detection_metrics': {'include_metrics_per_category': True}\n",
    "      }\n",
    "  \"\"\"\n",
    "  eval_metric_fn_keys = eval_config.metrics_set\n",
    "  evaluator_options = {}\n",
    "  for eval_metric_fn_key in eval_metric_fn_keys:\n",
    "    if eval_metric_fn_key in (\n",
    "        'coco_detection_metrics', 'coco_mask_metrics', 'lvis_mask_metrics'):\n",
    "      evaluator_options[eval_metric_fn_key] = {\n",
    "          'include_metrics_per_category': (\n",
    "              eval_config.include_metrics_per_category)\n",
    "      }\n",
    "\n",
    "      if (hasattr(eval_config, 'all_metrics_per_category') and\n",
    "          eval_config.all_metrics_per_category):\n",
    "        evaluator_options[eval_metric_fn_key].update({\n",
    "            'all_metrics_per_category': eval_config.all_metrics_per_category\n",
    "        })\n",
    "      # For coco detection eval, if the eval_config proto contains the\n",
    "      # \"skip_predictions_for_unlabeled_class\" field, include this field in\n",
    "      # evaluator_options.\n",
    "      if eval_metric_fn_key == 'coco_detection_metrics' and hasattr(\n",
    "          eval_config, 'skip_predictions_for_unlabeled_class'):\n",
    "        evaluator_options[eval_metric_fn_key].update({\n",
    "            'skip_predictions_for_unlabeled_class':\n",
    "                (eval_config.skip_predictions_for_unlabeled_class)\n",
    "        })\n",
    "      for super_category in eval_config.super_categories:\n",
    "        if 'super_categories' not in evaluator_options[eval_metric_fn_key]:\n",
    "          evaluator_options[eval_metric_fn_key]['super_categories'] = {}\n",
    "        key = super_category\n",
    "        value = eval_config.super_categories[key].split(',')\n",
    "        evaluator_options[eval_metric_fn_key]['super_categories'][key] = value\n",
    "      if eval_metric_fn_key == 'lvis_mask_metrics' and hasattr(\n",
    "          eval_config, 'export_path'):\n",
    "        evaluator_options[eval_metric_fn_key].update({\n",
    "            'export_path': eval_config.export_path\n",
    "        })\n",
    "\n",
    "    elif eval_metric_fn_key == 'precision_at_recall_detection_metrics':\n",
    "      evaluator_options[eval_metric_fn_key] = {\n",
    "          'recall_lower_bound': (eval_config.recall_lower_bound),\n",
    "          'recall_upper_bound': (eval_config.recall_upper_bound)\n",
    "      }\n",
    "  return evaluator_options\n",
    "\n",
    "\n",
    "def has_densepose(eval_dict):\n",
    "  return (fields.DetectionResultFields.detection_masks in eval_dict and\n",
    "          fields.DetectionResultFields.detection_surface_coords in eval_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9c919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
