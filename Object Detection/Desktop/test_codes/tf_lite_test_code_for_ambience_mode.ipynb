{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9007fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import os\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3fbc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD_PATH = os.chdir('E:/ambience_mode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c9fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\ambience_mode\n"
     ]
    }
   ],
   "source": [
    "CWD_PATH = os.getcwd()\n",
    "print(CWD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3492c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required variables\n",
    "MODEL_NAME = 'models/'\n",
    "GRAPH_NAME = 'tflite_model_5_quant.tflite'\n",
    "LABELMAP_NAME = 'labelmap.txt'\n",
    "VIDEO_NAME = 'video_test/video_test_2.mp4'\n",
    "min_conf_threshold = float(0.7)\n",
    "use_TPU = None\n",
    "\n",
    "# Import TensorFlow libraries\n",
    "# If tflite_runtime is installed, import interpreter from tflite_runtime, else import from regular tensorflow\n",
    "# If using Coral Edge TPU, import the load_delegate library\n",
    "pkg = importlib.util.find_spec('tflite_runtime')\n",
    "if pkg:\n",
    "    from tflite_runtime.interpreter import Interpreter\n",
    "    if use_TPU:\n",
    "        from tflite_runtime.interpreter import load_delegate\n",
    "else:\n",
    "    from tensorflow.lite.python.interpreter import Interpreter\n",
    "    if use_TPU:\n",
    "        from tensorflow.lite.python.interpreter import load_delegate\n",
    "\n",
    "# If using Edge TPU, assign filename for Edge TPU model\n",
    "if use_TPU:\n",
    "    # If user has specified the name of the .tflite file, use that name, otherwise use default 'edgetpu.tflite'\n",
    "    if (GRAPH_NAME == 'detect.tflite'):\n",
    "        GRAPH_NAME = 'edgetpu.tflite'   \n",
    "\n",
    "# Get path to current working directory\n",
    "CWD_PATH = os.getcwd()\n",
    "\n",
    "# Path to video file\n",
    "VIDEO_PATH = os.path.join(CWD_PATH,VIDEO_NAME)\n",
    "\n",
    "# Path to .tflite file, which contains the model that is used for object detection\n",
    "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,GRAPH_NAME)\n",
    "\n",
    "# Path to label map file\n",
    "PATH_TO_LABELS = os.path.join(CWD_PATH,MODEL_NAME,LABELMAP_NAME)\n",
    "\n",
    "# Load the label map\n",
    "with open(PATH_TO_LABELS, 'r') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Have to do a weird fix for label map if using the COCO \"starter model\" from\n",
    "# https://www.tensorflow.org/lite/models/object_detection/overview\n",
    "# First label is '???', which has to be removed.\n",
    "if labels[0] == '???':\n",
    "    del(labels[0])\n",
    "\n",
    "# Load the Tensorflow Lite model.\n",
    "# If using Edge TPU, use special load_delegate argument\n",
    "if use_TPU:\n",
    "    interpreter = Interpreter(model_path=PATH_TO_CKPT,\n",
    "                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\n",
    "    print(PATH_TO_CKPT)\n",
    "else:\n",
    "    interpreter = Interpreter(model_path=PATH_TO_CKPT)\n",
    "\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f67c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale: 0.00390625 | Zero Point: 0\n"
     ]
    }
   ],
   "source": [
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "output_details[0]\n",
    "scale,zero_point = output_details[0]['quantization']\n",
    "print('Scale: '+str(scale)+' | Zero Point: '+str(zero_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19ee8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH\n",
    "min_conf_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "403ac5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11328125 0.03515625 0.1953125  0.65625   ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.12109375 0.03125    0.1953125  0.65234375]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.12109375 0.03125    0.1953125  0.65234375]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.11328125 0.0390625  0.18359375 0.6640625 ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.12109375 0.03125    0.2109375  0.63671875]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.15625    0.03515625 0.23046875 0.578125  ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.0390625  0.046875   0.1953125  0.71484375]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.046875   0.05078125 0.19140625 0.70703125]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.0703125  0.046875   0.17578125 0.70703125]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.0859375  0.04296875 0.21484375 0.65625   ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.15234375 0.06640625 0.26953125 0.51171875]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.140625   0.08203125 0.15625    0.62109375]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.08984375 0.0390625  0.0625     0.8125    ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.10546875 0.03515625 0.1171875  0.7421875 ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.1171875  0.04296875 0.10546875 0.73828125]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.16796875 0.046875   0.16796875 0.6171875 ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.35546875 0.04296875 0.171875   0.4296875 ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.4296875  0.046875   0.16796875 0.35546875]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.5        0.03125    0.09375    0.37890625]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.27734375 0.0625     0.17578125 0.484375  ]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.3359375  0.0390625  0.1328125  0.48828125]\n",
      "***\n",
      "******\n",
      "****\n",
      "[0.53515625 0.03515625 0.1484375  0.28125   ]\n",
      "***\n",
      "******\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "floating_model = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "input_mean = 127.5\n",
    "input_std = 127.5\n",
    "\n",
    "# Open video file\n",
    "video = cv2.VideoCapture(VIDEO_PATH)\n",
    "imW = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "imH = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "# Loop over every image and perform detection\n",
    "while (True):\n",
    "\n",
    "    # Acquire frame and resize to expected shape [1xHxWx3]\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "      print('Reached the end of the video!')\n",
    "      break\n",
    "    \n",
    "    # Load frame and resize to expected shape [1xHxWx3]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_resized = cv2.resize(frame_rgb, (width, height))\n",
    "    input_data = np.expand_dims(frame_resized, axis=0)\n",
    "    frameH, frameW = int(imH),int(imW) \n",
    "\n",
    "    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "    if floating_model:\n",
    "        input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "    # Perform the actual detection by running the model with the image as input\n",
    "    interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Retrieve detection results\n",
    "    #boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
    "    #classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n",
    "    scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n",
    "    #num = interpreter.get_tensor(output_details[3]['index'])[0]  # Total number of detected objects (inaccurate and not needed)\n",
    "    scores = (scores-zero_point)*scale\n",
    "    print(scores)\n",
    "    \n",
    "    label_index = np.argmax(scores, axis=0)\n",
    "    #predicted_label.append(labels[label_index])\n",
    "    print(\"***\")\n",
    "    # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "    for i in range(len(scores)):\n",
    "        if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\n",
    "            print(\"******\")\n",
    "            # Get bounding box coordinates and draw box\n",
    "            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "            ymin = 3\n",
    "            xmin = 3\n",
    "            ymax = frameH-3\n",
    "            xmax = frameW-3\n",
    "            \n",
    "            cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "\n",
    "            # Draw label\n",
    "            object_name = labels[label_index] # Look up object name from \"labels\" array using class index\n",
    "            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "            cv2.rectangle(frame, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "    print(\"****\")\n",
    "    # All the results have been drawn on the frame, so it's time to display it.\n",
    "    cv2.imshow('Object detector', frame)\n",
    "\n",
    "    # Press any key to continue to next image, or press 'q' to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025bcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
